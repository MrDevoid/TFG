{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trabajo de fin de grado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se importan las librerias necesarias, en primer lugar algunas librerias para tratar con los tipos de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se importan tambien los algoritmos que se utilizaran más adelante para predecir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection  import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.naive_bayes import GaussianNB \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Librerias para el preprocesado y analisis de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Utilizados para normalizar\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "#Para la seleccion de variables\n",
    "import sklearn.feature_selection\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y otras de ayuda para plotear gráficas y mostrar métricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import itertools\n",
    "from sklearn.metrics import confusion_matrix, precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datos de audios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se comienza con la carga de los archivos .csv que han de encontrarse en el mismo directorio. Como se ve en el código cuando el valor no está definido aparece *--undefined--* y por ello esos valores son tratados como NA's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "audiosMezclados = pd.read_csv('sonidosMezclados.csv', na_values='--undefined--')\n",
    "audiosAleman = pd.read_csv('sonidosAleman.csv', na_values='--undefined--')\n",
    "audiosEspanol = pd.read_csv('sonidosEspanol.csv', na_values='--undefined--')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observando los cantidad de audios de las diferentes clases que se encuentran en cada uno de los dataframes, es fácil ver que en dataframe de audios en Español hay una presencia muy elevada de audios neutrales, por ello a continuación se eliminan varios de ellos de forma que finalmente todos estén presentes en la misma proporción"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neutrales = audiosEspanol[audiosEspanol['Result']==5]\n",
    "audiosEspanol = audiosEspanol.drop(np.arange((neutrales.index[0]+184),neutrales.index[-1]+1))\n",
    "audiosEspanol = audiosEspanol.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación se separan los dataframes en dos, uno con los datos de entrenamiento y otro con la etiqueta de cada ejemplo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_Mezclados = audiosMezclados.drop(['Result'], axis=1)\n",
    "y_Mezclados = audiosMezclados.Result\n",
    "\n",
    "X_Aleman = audiosAleman.drop(['Result'], axis=1)\n",
    "y_Aleman = audiosAleman.Result\n",
    "\n",
    "X_Espanol = audiosEspanol.drop(['Result'], axis=1)\n",
    "y_Espanol = audiosEspanol.Result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se definen unas funciones utiles para normalizar los datos de entrenamiento y las pruebas que se realizan más adelante; junto con otras para eliminar valores faltantes con la media."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcularMediaYDesviacion(df):\n",
    "    mean = df.mean(numeric_only=1)\n",
    "    std = df.std(numeric_only=1)\n",
    "    return mean,std\n",
    "def normalizar(df, mean, std):\n",
    "    df = np.subtract(df,np.expand_dims(mean,axis=0))\n",
    "    df = np.divide(df,np.expand_dims(std,axis=0))\n",
    "    return df\n",
    "def normalizarPrueba(df,mean,std):\n",
    "    df = np.subtract(df,np.expand_dims(mean,axis=0))\n",
    "    df = np.divide(df,np.expand_dims(std,axis=0))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deleteNAWithMean(df,mean):\n",
    "    for i in df.columns.values:\n",
    "        df[i].replace(np.nan, mean[i], inplace = True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dado que existen varios valores nulos, se reemplazan con la media de la columna a la que pertenecen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meanMezclados, stdMezclados = calcularMediaYDesviacion(X_Mezclados)\n",
    "meanAleman, stdAleman = calcularMediaYDesviacion(X_Aleman)\n",
    "meanEspanol, stdEspanol = calcularMediaYDesviacion(X_Espanol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_Mezclados = deleteNAWithMean(X_Mezclados,meanMezclados)\n",
    "X_Aleman = deleteNAWithMean(X_Aleman,meanAleman)\n",
    "X_Espanol = deleteNAWithMean(X_Espanol,meanEspanol)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para realizar un analisis sobre la correlación que existe entre las variables, se muestran algunos mapas de calor en los que existe al menos un par de variables con una correlación absoluta mayor a 0.8, para después valorar la eliminación de alguna de esas variables con la finalidad de reducir los datos de entrenamiento necesarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "correlation = X_Mezclados.corr()\n",
    "limites = [0,5,10,15,21,23,28,33,38,43,48,53,58,63,68,73,78,83,88,94,100,106,112,118]\n",
    "for i in range(0,len(limites)-1):\n",
    "    for j in range(i,len(limites)-1):\n",
    "        correlation = correlation.iloc[limites[i]:limites[i+1],limites[j]:limites[j+1]]\n",
    "        if(np.any(correlation<-0.8) or np.any(correlation>0.8)):\n",
    "            plt.figure()\n",
    "            sns.heatmap(correlation, xticklabels = correlation.columns.values,\n",
    "                        yticklabels=correlation.index.values, vmin=-1,vmax=1,annot=True)\n",
    "            plt.title('Feature correlations')\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(correlation.columns.values[0] + correlation.index.values[0])\n",
    "        correlation = X_Mezclados.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalizar datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se definen varios métodos para normalizar con el objetivo de poder comparar los resultados que producen cada una de ellas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalización utilizando el método MinMax, que convierte los valores a un intervalo de 0-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "scalerMezclados = scaler.fit(X_Mezclados)\n",
    "scalerAleman = scaler.fit(X_Aleman)\n",
    "scalerEspanol = scaler.fit(X_Espanol)\n",
    "minMaxScaled_Mezclados = scalerMezclados.transform(X_Mezclados)\n",
    "minMaxScaled_Aleman = scalerAleman.transform(X_Aleman)\n",
    "minMaxScaled_Espanol = scalerEspanol.transform(X_Espanol)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalización ZScore, para transformar los datos a una normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalizar_Mezclados = normalizar(X_Mezclados,meanMezclados,stdMezclados)\n",
    "normalizar_Aleman = normalizar(X_Aleman,meanAleman,stdAleman)\n",
    "normalizar_Espanol = normalizar(X_Espanol,meanEspanol,stdEspanol)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalización utilizando la norma l1, l2 o max, según se especifique en los argumentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_Mezclados, normMezclados = normalize(X_Mezclados, norm = 'l2',axis=0,return_norm=True)\n",
    "normalized_Aleman, normAleman = normalize(X_Aleman, norm='l2',axis=0,return_norm=True)\n",
    "normalized_Espanol, normEspanol = normalize(X_Espanol, norm='l2',axis=0,return_norm=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalización realizada con el rango interquartílico, de forma que sea más robusta frente a valores atípicos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "robustScaler = RobustScaler()\n",
    "robustScalerMezclados = robustScaler.fit(X_Mezclados)\n",
    "robustScalerAleman = robustScaler.fit(X_Aleman)\n",
    "robustScalerEspanol = robustScaler.fit(X_Espanol)\n",
    "robustScaled_Mezclados = robustScalerMezclados.transform(X_Mezclados)\n",
    "robustScaled_Aleman = robustScalerAleman.transform(X_Aleman)\n",
    "robustScaled_Espanol = robustScalerEspanol.transform(X_Espanol)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seleccionar datos para entrenar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para facilitar la selección de los datos se ponen a disposicion asignaciones de todas las normalizaciones de los datos, de forma que bastará con descomentar las asignaciones que se quieran realizar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#opN=1\n",
    "#X_MezcladosNor, scalerM = pd.DataFrame(minMaxScaled_Mezclados, columns = X_Mezclados.columns.values), scalerMezclados\n",
    "#X_AlemanNor, scalerA = pd.DataFrame(minMaxScaled_Aleman, columns = X_Aleman.columns.values), scalerAleman\n",
    "#X_EspanolNor, scalerE = pd.DataFrame(minMaxScaled_Espanol, columns = X_Espanol.columns.values), scalerEspanol\n",
    "\n",
    "opN=2\n",
    "X_MezcladosNor = pd.DataFrame(normalizar_Mezclados)\n",
    "X_AlemanNor = pd.DataFrame(normalizar_Aleman)\n",
    "X_EspanolNor = pd.DataFrame(normalizar_Espanol)\n",
    "\n",
    "#opN=3\n",
    "#X_MezcladosNor = pd.DataFrame(normalized_Mezclados, columns = X_Mezclados.columns.values)\n",
    "#X_AlemanNor = pd.DataFrame(normalized_Aleman, columns = X_Aleman.columns.values)\n",
    "#X_EspanolNor = pd.DataFrame(normalized_Espanol, columns = X_Espanol.columns.values)\n",
    "\n",
    "#opN=4\n",
    "#X_MezcladosNor, scalerM = pd.DataFrame(robustScaled_Mezclados, columns = X_Mezclados.columns.values), robustScalerMezclados\n",
    "#X_AlemanNor, scalerA = pd.DataFrame(robustScaled_Aleman, columns = X_Aleman.columns.values), robustScalerAleman\n",
    "#X_EspanolNor, scalerE = pd.DataFrame(robustScaled_Espanol, columns = X_Espanol.columns.values), robustScalerEspanol"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seleccion de variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para seleccionar variables a eliminar, a parte de utilizar el analisis que se hizo anteriormente en base a las correlaciones; se lanza un algoritmo de decisión para que establezca un orden en la importancia de las variables. Así las variables que el arbol considere menos importantes se podrán eliminar, al igual que las que considere más importantes se procurará mantenerlas "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model = ExtraTreesClassifier(n_estimators=90)\n",
    "model = model.fit(X_MezcladosNor,y_Mezclados)\n",
    "featureImportance = model.feature_importances_\n",
    "print(featureImportance)\n",
    "featureImportance = np.argsort(featureImportance)\n",
    "print(featureImportance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tras entrenar con el arbol, se implementa un metodo que devuelve las variables más o menos importantes, en función del signo del parámetro de entrada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def moreImportantFeatures(how_many, classifier, inputData, outputData):\n",
    "    model = classifier.fit(inputData,outputData)\n",
    "    featureImportance = model.feature_importances_\n",
    "    orderImportance = np.argsort(featureImportance)\n",
    "    if(how_many >=0):\n",
    "        for i in range(1,how_many):\n",
    "            print(inputData.columns.values[orderImportance[-i]])\n",
    "    else:\n",
    "        for i in range(1,-how_many):\n",
    "            print(inputData.columns.values[orderImportance[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moreImportantFeatures(-20, ExtraTreesClassifier(n_estimators=90),X_MezcladosNor,y_Mezclados)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación se crean varios conjuntos eliminando algunas variables. En uno de ellos eliminamos variables que tenian más de un 0.8 de correlación absoluta; otro eliminando aquellas variables que el arbol de decisión determina que son menos influyentes y un ultimo eliminando variables que cumplan alguna de las condiciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_MezcladosNor80 = X_MezcladosNor.drop(['meanEnergy','minEnergy','rangeEnergy','stdEnergy','rangeAmplitude','minMFCC1','stdMFCC1','rangeMFCC2','rangeMFCC4','rangeMFCC12','maxPitch','stdPitch','quantileFormant1','quantileFormant2','rangeFormant2','quantileFormant3','quantileFormant4','rangeFormant4','rangeFormant5','quantileFormant5','stdAmplitude','articulationRate','meanIntensity'], axis=1)\n",
    "X_AlemanNor80 = X_AlemanNor.drop(['meanEnergy','minEnergy','rangeEnergy','stdEnergy','rangeAmplitude','minMFCC1','stdMFCC1','rangeMFCC2','rangeMFCC4','rangeMFCC12','maxPitch','stdPitch','quantileFormant1','quantileFormant2','rangeFormant2','quantileFormant3','quantileFormant4','rangeFormant4','rangeFormant5','quantileFormant5','stdAmplitude','articulationRate','meanIntensity'], axis=1)\n",
    "X_EspanolNor80 = X_EspanolNor.drop(['meanEnergy','minEnergy','rangeEnergy','stdEnergy','rangeAmplitude','minMFCC1','stdMFCC1','rangeMFCC2','rangeMFCC4','rangeMFCC12','maxPitch','stdPitch','quantileFormant1','quantileFormant2','rangeFormant2','quantileFormant3','quantileFormant4','rangeFormant4','rangeFormant5','quantileFormant5','stdAmplitude','articulationRate','meanIntensity'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_MezcladosNorLessImportant = X_MezcladosNor.drop(['minFormant1','maxFormant3','minHarmonicity','maxFormant5','rangeFormant2','rangeFormant1','maxMFCC6','maxFormant2','minFormant2','maxMFCC4','maxFormant4'], axis=1)\n",
    "X_AlemanNorLessImportant = X_AlemanNor.drop(['minFormant1','maxFormant3','minHarmonicity','maxFormant5','rangeFormant2','rangeFormant1','maxMFCC6','maxFormant2','minFormant2','maxMFCC4','maxFormant4'], axis=1)\n",
    "X_EspanolNorLessImportant = X_EspanolNor.drop(['minFormant1','maxFormant3','minHarmonicity','maxFormant5','rangeFormant2','rangeFormant1','maxMFCC6','maxFormant2','minFormant2','maxMFCC4','maxFormant4'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_MezcladosNor80LessImportant = X_MezcladosNor80.drop(['minFormant1','maxFormant3','minHarmonicity','maxFormant5','rangeFormant1','maxMFCC6','maxFormant2','minFormant2','maxMFCC4','maxFormant4'], axis=1)\n",
    "X_AlemanNor80LessImportant = X_AlemanNor80.drop(['minFormant1','maxFormant3','minHarmonicity','maxFormant5','rangeFormant1','maxMFCC6','maxFormant2','minFormant2','maxMFCC4','maxFormant4'], axis=1)\n",
    "X_EspanolNor80LessImportant = X_EspanolNor80.drop(['minFormant1','maxFormant3','minHarmonicity','maxFormant5','rangeFormant1','maxMFCC6','maxFormant2','minFormant2','maxMFCC4','maxFormant4'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def histogramas(data):\n",
    "    for nameOfColumn in data.columns.values:\n",
    "        data.hist(nameOfColumn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "histogramas(X_Mezclados)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nuevamente tras finalizar esta fase se dispone una serie de opciones para determinar con que conjunto se procedera a la eliminación de outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#opE =1\n",
    "#X_MezcladosReduced = X_MezcladosNor\n",
    "#X_AlemanReduced = X_AlemanNor\n",
    "#X_EspanolReduced = X_EspanolNor\n",
    "\n",
    "#opE=2\n",
    "#X_MezcladosReduced = X_MezcladosNor80\n",
    "#X_AlemanReduced = X_AlemanNor80\n",
    "#X_EspanolReduced = X_EspanolNor80\n",
    "\n",
    "#opE=3\n",
    "#X_MezcladosReduced = X_MezcladosNorLessImportant\n",
    "#X_AlemanReduced = X_AlemanNorLessImportant\n",
    "#X_EspanolReduced = X_EspanolNorLessImportant\n",
    "\n",
    "opE=4\n",
    "X_MezcladosReduced = X_MezcladosNor80LessImportant\n",
    "X_AlemanReduced = X_AlemanNor80LessImportant\n",
    "X_EspanolReduced = X_EspanolNor80LessImportant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supresion de outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para eliminar los outliers se define una función que calcula el primer y tercer cuartil y con ello el rango intercuartílico. En este caso, dado que el porcentaje de outliers inicial era demasiado elevado, el valor que multiplica al rango intercualtílico ha sido aumentado para ser más flexibles a la hora de determinar si un valor es atípico."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deleteOutliers(data,outdata):\n",
    "    lowerOutliers = np.array([False])\n",
    "    upperOutliers = np.array([False])\n",
    "    for i in data.columns.values:\n",
    "        quartiles = data[i].quantile([0.25,0.75])\n",
    "        IQR = quartiles[0.75]-quartiles[0.25]\n",
    "        lowerLimit = quartiles[0.25] - 2.5*IQR\n",
    "        upperLimit = quartiles[0.75] + 2.5*IQR\n",
    "        lowerOutliersAux = data[i].values < lowerLimit\n",
    "        upperOutliersAux = data[i].values > upperLimit\n",
    "        lowerOutliers = np.logical_or(lowerOutliers, lowerOutliersAux)\n",
    "        upperOutliers = np.logical_or(upperOutliers, upperOutliersAux)\n",
    "    outliers = np.logical_or(lowerOutliers, upperOutliers)\n",
    "    data = data.drop(np.where(outliers)[0])\n",
    "    outdata = outdata.drop(np.where(outliers)[0])\n",
    "    data = data.reset_index(drop=True)\n",
    "    outdata = outdata.reset_index(drop=True)\n",
    "    return data, outdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_MezcladosNorWithoutOutliers,y_MezcladosWithoutOutliers=deleteOutliers(X_MezcladosReduced,y_Mezclados)\n",
    "X_AlemanNorWithoutOutliers, y_AlemanWithoutOutliers=deleteOutliers(X_AlemanReduced, y_Aleman)\n",
    "X_EspanolNorWithoutOutliers, y_EspanolWithoutOutliers=deleteOutliers(X_EspanolReduced, y_Espanol)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datos de actividad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cargamos de scores.csv los datos de los 22 pacientes a evaluar, quitando las columnas innecesarias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datosPacientes = pd.read_csv('depresjon-dataset/data/scores.csv', na_values='--undefined--')\n",
    "datosPacientes = datosPacientes.drop(columns=['number'])\n",
    "datosPacientes = datosPacientes.drop(columns=['days'])\n",
    "datosPacientes.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Número pacientes por generó y su clase iniciales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Pacientes género masculino:\", sum(datosPacientes.gender == 2))\n",
    "print(\"Pacientes género femenino:\", sum(datosPacientes.gender == 1))\n",
    "print(\"Pacientes con depresión:\", sum(datosPacientes.afftype == 2))\n",
    "print(\"Pacientes bipolares:\", sum(datosPacientes.afftype == 1))\n",
    "print(\"Pacientes género masculino y depresión:\", sum((datosPacientes.gender == 2) & (datosPacientes.afftype == 2)))\n",
    "print(\"Pacientes género femenino y depresión:\", sum((datosPacientes.gender == 1) & (datosPacientes.afftype == 2)))\n",
    "print(\"Pacientes género masculino y bipolares:\", sum((datosPacientes.gender == 2) & (datosPacientes.afftype == 1)))\n",
    "print(\"Pacientes género femenino y bipolares:\", sum((datosPacientes.gender == 1) & (datosPacientes.afftype == 1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seleccionamos por generos los pacientes tanto depresivos como bipolares para distribuir los datos con el mismo número de pacientes de cada tipo y género."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_type1_male = datosPacientes[(datosPacientes.afftype==1) & (datosPacientes.gender==2)] \n",
    "index_type1_female = datosPacientes[(datosPacientes.afftype==1) & (datosPacientes.gender==1)]\n",
    "index_type2_male = datosPacientes[(datosPacientes.afftype==2) & (datosPacientes.gender==2)]\n",
    "index_type2_female = datosPacientes[(datosPacientes.afftype==2) & (datosPacientes.gender==1)]\n",
    "\n",
    "remuve_n = 0\n",
    "\n",
    "if(len(index_type1_male) <= len(index_type2_male)):\n",
    "    remove_male = len(index_type2_male) - len(index_type1_male)\n",
    "    index_type2_male = index_type2_male.sample(len(index_type1_male))   \n",
    "else:\n",
    "    remove_male = len(index_type1_male) - len(index_type2_male)\n",
    "    index_type1_male = index_type1_male.sample(len(index_type2_male))\n",
    "if(len(index_type1_female) <= len(index_type2_female)):\n",
    "    remove_female = len(index_type2_female) - len(index_type1_female)\n",
    "    index_type2_female = index_type2_female.sample(len(index_type1_female))\n",
    "else:\n",
    "    remove_female = len(index_type1_female) - len(index_type2_female)\n",
    "    index_type1_female = index_type1_female.sample(len(index_type2_female))\n",
    "    \n",
    "remove_n = remove_male + remove_female"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datosPacientes = pd.concat([index_type1_male, index_type1_female])\n",
    "datosPacientes = pd.concat([datosPacientes, index_type2_male])\n",
    "datosPacientes = pd.concat([datosPacientes, index_type2_female])\n",
    "datosPacientes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Pacientes género masculino:\", sum(datosPacientes.gender == 2))\n",
    "print(\"Pacientes género femenino:\", sum(datosPacientes.gender == 1))\n",
    "print(\"Pacientes con depresión:\", sum(datosPacientes.afftype == 2))\n",
    "print(\"Pacientes bipolares:\", sum(datosPacientes.afftype == 1))\n",
    "print(\"Pacientes género masculino y depresión:\", sum((datosPacientes.gender == 2) & (datosPacientes.afftype == 2)))\n",
    "print(\"Pacientes género femenino y depresión:\", sum((datosPacientes.gender == 1) & (datosPacientes.afftype == 2)))\n",
    "print(\"Pacientes género masculino y bipolares:\", sum((datosPacientes.gender == 2) & (datosPacientes.afftype == 1)))\n",
    "print(\"Pacientes género femenino y bipolares:\", sum((datosPacientes.gender == 1) & (datosPacientes.afftype == 1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cargamos de la carpeta condition los datos de los archivos CSV de actividad correspondiente a cada paciente que hemos seleccionado, almacenandolo en un array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actividadPaciente = []\n",
    "for i in list(datosPacientes.index):\n",
    "    path = 'depresjon-dataset/data/condition/condition_' + str(i+1) + '.csv'\n",
    "    actividadPaciente.append(pd.read_csv(path, na_values='--undefined--'))\n",
    "datosPacientes = datosPacientes.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eliminamos la columna date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, 22 - remove_n):\n",
    "    actividadPaciente[i] = actividadPaciente[i].drop(columns=['date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seleccionamos el número de horas por las que vamos a agregar los datos de actividad medidos en cada paciente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hours = 6\n",
    "#hours = 2\n",
    "resample_hour = str(hours) + 'H'\n",
    "n_hour_class = int(24 / hours)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Realizamos la agregación cada resample_hour horas para calcular la media (meanActivity), suma (sumActivity), desviación típica (stdActivity), varianza (varActivity), maximo (max) y minimo (min) de la actividad de cada paciente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, 22 - remove_n):\n",
    "    actividadPaciente[i].index = pd.DatetimeIndex(actividadPaciente[i].timestamp)\n",
    "    aux = actividadPaciente.copy()\n",
    "    actividadPaciente[i] = round(aux[i].resample(resample_hour).mean(),2)\n",
    "    actividadPaciente[i]['sumActivity'] = round(aux[i].resample(resample_hour).sum(),2)\n",
    "    actividadPaciente[i]['stdActivity'] = round(aux[i].resample(resample_hour).std()['activity'],2)\n",
    "    actividadPaciente[i]['varActivity'] = round(aux[i].resample(resample_hour).var()['activity'],2)\n",
    "    actividadPaciente[i]['max'] = aux[i].resample(resample_hour).max()['activity']\n",
    "    actividadPaciente[i]['min'] = aux[i].resample(resample_hour).min()['activity']\n",
    "    actividadPaciente[i]=actividadPaciente[i].rename(columns = {'activity':'meanActivity'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para evitar incluir en los datos de actividad de cada paciente periodos al final donde la monitorización habia acabado y hay registros de actividad a 0, realizamos la siguiente operación para conseguir disminuir su porcentaje: Mientras que la suma de la actividad media del 20 % del final de los datos sea menor que la suma de la actividad media del 20 % del principio de los datos entre 3, vamos disminuyendo los datos un 5 % del final."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(0,22 - remove_n):\n",
    "    while(sum(actividadPaciente[j].meanActivity[math.trunc(len(actividadPaciente[j])*0.80):]) < sum(actividadPaciente[j].meanActivity[:math.trunc((len(actividadPaciente[j])*0.20)/3)])):\n",
    "        actividadPaciente[j] = actividadPaciente[j][:math.trunc(len(actividadPaciente[j])*0.95)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eliminamos las primeras horas hasta el primer registro que comience a las 00:00:00"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "while(i < len(actividadPaciente)):\n",
    "    while(actividadPaciente[i].index[0].hour != 0):\n",
    "        actividadPaciente[i] = actividadPaciente[i][1:]\n",
    "    i = i + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repetimos el proceso eliminando ahora las últimas horas hasta el primer registro anterior a las 00:00:00"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "limitHour = max(actividadPaciente[0].index.hour.values)\n",
    "while(i < len(actividadPaciente)):\n",
    "    while(actividadPaciente[i].index[-1].hour != limitHour):\n",
    "        actividadPaciente[i] = actividadPaciente[i][:-1]\n",
    "    i = i + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dejamos en la columna hour solamente la hora correspondiente al periodo de actividad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, 22 - remove_n):\n",
    "    actividadPaciente[i]['hour'] = actividadPaciente[i].index\n",
    "    actividadPaciente[i].hour = actividadPaciente[i].hour.astype(str).tolist()\n",
    "    actividadPaciente[i] = actividadPaciente[i].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, 22 - remove_n):\n",
    "    times = []\n",
    "    for elem in actividadPaciente[i].hour:\n",
    "        times.append(elem.split(' ')[1])\n",
    "    actividadPaciente[i].hour = times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En las siguientes graficas podemos ver el nivel de actividad por cada uno de los campos obtenidos para cada paciente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, 22 - remove_n):\n",
    "    actividadPaciente[i].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concatenamos los dataframes para que cada paciente tenga su correspondiente tabla de actividad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalDF = pd.DataFrame()\n",
    "for i in range(0, 22 - remove_n):\n",
    "    df1 = actividadPaciente[i]\n",
    "    df2 = datosPacientes[i:i+1].reindex(datosPacientes[i:i+1].index.repeat(len(actividadPaciente[i]))).reset_index(drop=True)\n",
    "    auxDF = pd.concat([df1,df2], axis=1)\n",
    "    finalDF = pd.concat([finalDF,auxDF]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalizamos todas las columnas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = preprocessing.LabelEncoder()\n",
    "finalDF.age = le.fit_transform(finalDF.age)\n",
    "finalDF.edu = le.fit_transform(finalDF.edu)\n",
    "finalDF.hour = le.fit_transform(finalDF.hour)\n",
    "finalDF = finalDF[~finalDF['stdActivity'].isnull()]\n",
    "X_activity = finalDF.drop(['afftype'], axis=1)\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "X_activity.meanActivity = scaler.fit_transform(np.expand_dims(X_activity.meanActivity, axis=1))\n",
    "X_activity.stdActivity = scaler.fit_transform(np.expand_dims(X_activity.stdActivity, axis=1))\n",
    "X_activity.madrs1 = scaler.fit_transform(np.expand_dims(X_activity.madrs1, axis=1))\n",
    "X_activity.madrs2 = scaler.fit_transform(np.expand_dims(X_activity.madrs2, axis=1))\n",
    "X_activity.varActivity = scaler.fit_transform(np.expand_dims(X_activity.varActivity, axis=1))\n",
    "X_activity.sumActivity = scaler.fit_transform(np.expand_dims(X_activity.sumActivity, axis=1))\n",
    "X_activity['max'] = scaler.fit_transform(np.expand_dims(X_activity['max'], axis=1))\n",
    "X_activity['min'] = scaler.fit_transform(np.expand_dims(X_activity['min'], axis=1))\n",
    "y_activity = finalDF.afftype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_activity.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eliminamos las columnas que hacen referencia a los datos del paciente y que no tienen relacion con la actividad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gender = X_activity.gender[X_activity.hour==0].reset_index(drop=True)\n",
    "age = X_activity.age[X_activity.hour==0].reset_index(drop=True)\n",
    "inpatient = X_activity.inpatient[X_activity.hour==0].reset_index(drop=True)\n",
    "edu = X_activity.edu[X_activity.hour==0].reset_index(drop=True)\n",
    "marriage = X_activity.marriage[X_activity.hour==0].reset_index(drop=True)\n",
    "work = X_activity.work[X_activity.hour==0].reset_index(drop=True)\n",
    "madrs1 = X_activity.madrs1[X_activity.hour==0].reset_index(drop=True)\n",
    "madrs2 = X_activity.madrs2[X_activity.hour==0].reset_index(drop=True)\n",
    "\n",
    "\n",
    "X_activity = X_activity.drop(['age'], axis=1)\n",
    "X_activity = X_activity.drop(['inpatient'], axis=1)\n",
    "X_activity = X_activity.drop(['edu'], axis=1)\n",
    "X_activity = X_activity.drop(['marriage'], axis=1)\n",
    "X_activity = X_activity.drop(['work'], axis=1)\n",
    "X_activity = X_activity.drop(['madrs1'], axis=1)\n",
    "X_activity = X_activity.drop(['madrs2'], axis=1)\n",
    "X_activity = X_activity.drop(['gender'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Graficas de días completos pacientes depresivos (Realizar con agregaciónes de 1 hora)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_activity[y_activity==2][:23].drop(columns=['hour','sumActivity', 'max', 'min', 'varActivity','stdActivity']).reset_index(drop=True).plot(ylim=[0, 0.35])\n",
    "#X_activity[y_activity==2][24:47].drop(columns=['hour', 'sumActivity', 'max', 'min', 'varActivity','stdActivity']).reset_index(drop=True).plot(ylim=[0, 0.35])\n",
    "#X_activity[y_activity==2][48:71].drop(columns=['hour', 'sumActivity', 'max', 'min', 'varActivity','stdActivity']).reset_index(drop=True).plot(ylim=[0, 0.35])\n",
    "#X_activity[y_activity==2][72:95].drop(columns=['hour', 'sumActivity', 'max', 'min', 'varActivity','stdActivity']).reset_index(drop=True).plot(ylim=[0, 0.35])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Graficas de días completos pacientes bipolares (Realizar con agregaciónes de 1 hora)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_activity[y_activity==1][:23].drop(columns=['hour', 'sumActivity', 'max', 'min', 'varActivity','stdActivity']).plot(ylim=[0, 0.35])\n",
    "#X_activity[y_activity==1][24:47].drop(columns=['hour', 'sumActivity', 'max', 'min', 'varActivity', 'stdActivity']).reset_index(drop=True).plot(ylim=[0, 0.35])\n",
    "#X_activity[y_activity==1][48:71].drop(columns=['hour', 'sumActivity', 'max', 'min', 'varActivity','stdActivity']).reset_index(drop=True).plot(ylim=[0, 0.35])\n",
    "#X_activity[y_activity==1][72:95].drop(columns=['hour', 'sumActivity', 'max', 'min', 'varActivity','stdActivity']).reset_index(drop=True).plot(ylim=[0, 0.35])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ponemos las filas correspondientes a un mismo día en columnas, para que cada fila represente un día entero de actividad de los pacientes, enumerandolas según el número de particiones que hagamos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = X_activity.drop(['hour'], axis=1).columns.values\n",
    "X_activity_list = []\n",
    "X_activity_final = X_activity[X_activity.hour==0].drop(['hour'], axis=1).reset_index(drop=True)\n",
    "y_activity_final = y_activity[X_activity.hour==0].reset_index(drop=True)\n",
    "X_activity_list.append(X_activity_final)\n",
    "for i in range(0, n_hour_class - 1):\n",
    "    X_activity_list.append(X_activity[X_activity.hour==i + 1].reset_index(drop=True))\n",
    "    X_activity_list[-1] = X_activity_list[-1].drop(['hour'], axis=1)\n",
    "\n",
    "for j in range(0, n_hour_class - 1):\n",
    "    X_activity_list[j+1].columns = list(columns + str(j + 1))\n",
    "    X_activity_final = pd.concat([X_activity_final, X_activity_list[j+1]], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Añadimos las columnas con información referida a los pacientes que deseemos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_activity_final = pd.concat([X_activity_final, gender], axis=1)\n",
    "#X_activity_final = pd.concat([X_activity_final, age], axis=1)\n",
    "#X_activity_final = pd.concat([X_activity_final, inpatient], axis=1)\n",
    "#X_activity_final = pd.concat([X_activity_final, edu], axis=1)\n",
    "#X_activity_final = pd.concat([X_activity_final, marriage], axis=1)\n",
    "#X_activity_final = pd.concat([X_activity_final, work], axis=1)\n",
    "#X_activity_final = pd.concat([X_activity_final, madrs1], axis=1)\n",
    "#X_activity_final = pd.concat([X_activity_final, madrs2], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eliminamos las columnas con datos referidos a la actividad que deseemos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for elem in [#'meanActivity', \n",
    "             #'sumActivity', \n",
    "             #'stdActivity', \n",
    "             #'varActivity', \n",
    "             #'max',\n",
    "             'min'\n",
    "            ]:\n",
    "    X_activity_final = X_activity_final.drop([elem], axis=1)\n",
    "\n",
    "for i in range(1, n_hour_class):\n",
    "    #X_activity_final = X_activity_final.drop(['meanActivity' + str(i)], axis=1)\n",
    "    #X_activity_final = X_activity_final.drop(['sumActivity' + str(i)], axis=1)\n",
    "    #X_activity_final = X_activity_final.drop(['stdActivity' + str(i)], axis=1)\n",
    "    #X_activity_final = X_activity_final.drop(['varActivity' + str(i)], axis=1)\n",
    "    #X_activity_final = X_activity_final.drop(['max' + str(i)], axis=1)\n",
    "    X_activity_final = X_activity_final.drop(['min' + str(i)], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparación medias de dos tipos de pacientes por tramos del día"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activity = pd.concat([X_activity_final, y_activity_final], axis=1)\n",
    "prueba1 = []\n",
    "prueba2 = []\n",
    "resul = []\n",
    "for i in range(0, n_hour_class):\n",
    "    if i == 0:\n",
    "        aux1 = activity[activity['afftype'] == 1]\n",
    "        aux2 = activity[activity['afftype'] == 2]\n",
    "        prueba1.append(aux1['meanActivity'].mean())\n",
    "        prueba2.append(aux2['meanActivity'].mean())\n",
    "    else:\n",
    "        aux1 = activity[activity['afftype'] == 1]\n",
    "        aux2 = activity[activity['afftype'] == 2]\n",
    "        prueba1.append(aux1['meanActivity' + str(i)].mean())\n",
    "        prueba2.append(aux2['meanActivity' + str(i)].mean())\n",
    "    resul.append([prueba1[i] - prueba2[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(0, n_hour_class), resul)\n",
    "plt.ylabel('diferencia meanActivity (bipolar - depresión)')\n",
    "plt.xlabel('horas')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prueba1 = []\n",
    "prueba2 = []\n",
    "resul = []\n",
    "for i in range(0, n_hour_class):\n",
    "    if i == 0:\n",
    "        aux1 = X_activity_final[X_activity_final.gender==1]\n",
    "        aux2 = X_activity_final[X_activity_final.gender==2]\n",
    "        prueba1.append(aux1['meanActivity'].mean())\n",
    "        prueba2.append(aux2['meanActivity'].mean())\n",
    "    else:\n",
    "        aux1 = X_activity_final[X_activity_final.gender==1]\n",
    "        aux2 = X_activity_final[X_activity_final.gender==2]\n",
    "        prueba1.append(aux1['meanActivity' + str(i)].mean())\n",
    "        prueba2.append(aux2['meanActivity' + str(i)].mean())\n",
    "    resul.append([prueba1[i] - prueba2[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(0, n_hour_class), resul)\n",
    "plt.ylabel('diferencia actividad por genero (female - male)')\n",
    "plt.xlabel('horas')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Total días monitorizados por tipo paciente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Total días medidos\", len(X_activity_final))\n",
    "print(\"Total días medidos pacientes bipolares\", len(X_activity_final[y_activity_final==1]))\n",
    "print(\"Total días medidos pacientes depresión\", len(X_activity_final[y_activity_final==2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selección de datos para las pruebas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente se decide con que conjunto de datos se procede a entrenar y validar los algoritmos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Datos de audio\n",
    "#X_Train, y_Train, mean, std = X_MezcladosReduced,y_Mezclados, meanMezclados, stdMezclados\n",
    "#X_Train, y_Train, mean, std = X_AlemanReduced,y_Aleman, meanAleman, stdAleman\n",
    "#X_Train, y_Train, mean, std = X_EspanolReduced,y_Espanol, meanEspanol, stdEspanol\n",
    "#X_Train, y_Train, mean, std = X_MezcladosNorWithoutOutliers,y_MezcladosWithoutOutliers, meanMezclados, stdMezclados\n",
    "#X_Train, y_Train, mean, std = X_AlemanNorWithoutOutliers,y_AlemanWithoutOutliers, meanAleman, stdAleman\n",
    "X_Train, y_Train, mean, std = X_EspanolNorWithoutOutliers,y_EspanolWithoutOutliers, meanEspanol, stdEspanol\n",
    "\n",
    "#En caso de utilizar un metodo de normalización que se realice mediante un scaler\n",
    "#scaler = scalerM\n",
    "#scaler = scalerA\n",
    "scaler = scalerE\n",
    "\n",
    "#Datos de actividad\n",
    "#X_Train, y_Train = X_activity_final, y_activity_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para agregar las 6 clases en 3, una con las emociones alegria, ira y miedo que representaría la euforia o mania, otra con las emociones tristeza y asco que representaría la depresión y otra neutral, se modifican las etiquetas para tener 3: 0-euforia, 1-depresion, 2-neutral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_Train[y_Train==2]=0 #El miedo pasa a euforia\n",
    "y_Train[y_Train==3]=1 #El asco pasa a depresion\n",
    "y_Train[y_Train==4]=0 #La ira pasa a euforia\n",
    "y_Train[y_Train==5]=2 #Para tener las etiquetas en un orden creciente y continuo la etiqueta neutral pasa a ser un 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pruebas para determinar valores adecuados para cada algoritmo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bestNumberOfEstimatorsGBC = 0\n",
    "maxValueGBC = 0.0\n",
    "for i in range(1, 101, 10):\n",
    "    ini = time.time()\n",
    "    model = GradientBoostingClassifier(random_state=1, n_estimators=i)\n",
    "    scores = cross_val_score(model, X_Train, y_Train, cv=10, scoring='accuracy')\n",
    "    fin = time.time()\n",
    "    if scores.mean() > maxValueGBC:\n",
    "        maxValueGBC = scores.mean()\n",
    "        bestNumberOfEstimatorsGBC = i\n",
    "    print('Numero estimadores' , i)\n",
    "    print(scores.mean())\n",
    "    print('Tiempo estimado:', fin-ini, 'segundos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bestNumberOfEstimatorsRFC = 0\n",
    "maxValueRFC = 0\n",
    "for i in range(1, 101, 10):\n",
    "    ini = time.time()\n",
    "    model = RandomForestClassifier(n_estimators=i)\n",
    "    scores = cross_val_score(model, X_Train, y_Train, cv=10, scoring='accuracy')\n",
    "    fin = time.time()\n",
    "    if scores.mean() > maxValueRFC:\n",
    "        maxValueRFC = scores.mean()\n",
    "        bestNumberOfEstimatorsRFC = i\n",
    "    print('Numero estimadores' , i)\n",
    "    print(scores.mean())\n",
    "    print('Tiempo estimado:', fin-ini, 'segundos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bestNumNeuronas1 = 0\n",
    "bestAlpha = 0\n",
    "bestLearning_rate = 0\n",
    "maxValueMLP = 0\n",
    "for capa1 in range(10, 100, 10):\n",
    "    for j in [0.1, 0.01, 0.001, 1,10,100]:\n",
    "        ini = time.time()\n",
    "        model = MLPClassifier(hidden_layer_sizes=[capa1], activation='relu',solver='lbfgs',alpha=j,learning_rate='adaptive')\n",
    "        scores = cross_val_score(model, X_Train, y_Train, cv=10, scoring='accuracy')\n",
    "        fin = time.time()\n",
    "        if scores.mean() > maxValueMLP:\n",
    "            maxValueMLP = scores.mean()\n",
    "            bestNumNeuronas1 = capa1\n",
    "            bestAlpha = j\n",
    "        print('Neuronas capa oculta: ',capa1,'alpha: ',j,':  ',scores.mean())\n",
    "        print('Tiempo estimado:', fin-ini, 'segundos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ini = time.time()\n",
    "model = GaussianNB([0.17,0.17,0.17,0.17,0.16,0.16])\n",
    "scores = cross_val_score(model,X_Train,y_Train,cv=10,scoring='accuracy')\n",
    "fin = time.time()\n",
    "print(scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "mejorC = 0\n",
    "mejorGamma = 0\n",
    "maxPredictSVM = 0.0\n",
    "for i in [1, 10, 100, 1000]:\n",
    "    for j in [0.1, 0.01, 0.001, 1]:\n",
    "        ini = time.time()\n",
    "        model = SVC(kernel='rbf', C=i, gamma=j)\n",
    "        scores = cross_val_score(model, X_Train, y_Train, cv=10, scoring='accuracy')\n",
    "        fin = time.time()\n",
    "        if scores.mean() > maxPredictSVM:\n",
    "            maxPredictSVM = scores.mean()\n",
    "            mejorC = i\n",
    "            mejorGamma = j\n",
    "        print('C =', i, 'gamma =', j, ':', scores.mean())\n",
    "        print('Tiempo estimado:', fin-ini, 'segundos')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creacion y entrenamiento de algoritmos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#modelRandom = RandomForestClassifier(n_estimators=bestNumberOfEstimatorsRFC)\n",
    "#modelSVC = SVC(kernel='rbf', C=mejorC, gamma=mejorGamma)\n",
    "#modelGBC = GradientBoostingClassifier(random_state=1, n_estimators=bestNumberOfEstimatorsGBC)\n",
    "#modelMLP =  MLPClassifier(hidden_layer_sizes=[bestNumNeuronas1], activation='relu',solver='lbfgs',alpha=bestAlpha,learning_rate='adaptative')\n",
    "modelMLP =  MLPClassifier(hidden_layer_sizes=[20], activation='relu',solver='lbfgs',alpha=0.1,learning_rate='adaptive')\n",
    "#modelNB = GaussianNB([0.17,0.17,0.17,0.17,0.16,0.16])\n",
    "#modelRandom.fit(X_Train,y_Train)\n",
    "#modelSVC.fit(X_Train,y_Train)\n",
    "#modelGBC.fit(X_Train,y_Train)\n",
    "#modelMLP.fit(X_Train,y_Train)\n",
    "#modelNB.fit(X_Train,y_Train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pruebas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_Train, y_Train, random_state=0)\n",
    "\n",
    "#Para cuando los datos de prueba son audios\n",
    "#class_names = ['Euforia', 'Depresion', 'Neutral']\n",
    "class_names = ['Alegría', 'Tristeza', 'Miedo','Asco','Ira','Neutral']\n",
    "\n",
    "#Para cuando los datos de prueba son de actividad\n",
    "#class_names = ['bipolar', 'unipolar_depressive']\n",
    "\n",
    "#y_pred = modelSVC.fit(X_train, y_train).predict(X_test)\n",
    "#y_pred = modelRandom.fit(X_train, y_train).predict(X_test)\n",
    "#y_pred = modelGBC.fit(X_train, y_train).predict(X_test)\n",
    "#y_pred = modelNB.fit(X_train,y_train).predict(X_test)\n",
    "y_pred = modelMLP.fit(X_train,y_train).predict(X_test)\n",
    "\n",
    "\n",
    "precision, recall, f1Score, _ =precision_recall_fscore_support(y_test,y_pred)\n",
    "\n",
    "def plot_confusion_matrix(cm, classes, \n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()\n",
    "\n",
    "\n",
    "# Compute confusion matrix\n",
    "cnf_matrix = confusion_matrix(y_test, y_pred)\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=class_names,\n",
    "                      title='Confusion matrix, without normalization')\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=class_names, normalize=True,\n",
    "                      title='Normalized confusion matrix')\n",
    "\n",
    "plt.show()\n",
    "print('Precision: ',precision)\n",
    "print('Recall: ',recall)\n",
    "print('F1 Score: ',f1Score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pruebas de audio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por ultimo para realizar pruebas será necesario tratar los datos de prueba tal cuál se han tratado los datos de entrenamiento, por ello la función posee un parametro que indica que proceso se ha seguido y por lo tanto como a de tratar estos datos de prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformarDatos(datosPrueba, normalizacion, eliminacion):\n",
    "    datosPrueba = deleteNAWithMean(datosPrueba,mean)\n",
    "    if normalizacion == 1 or normalizacion == 4:\n",
    "        datosPrueba = scaler.transform(datosPrueba)\n",
    "    elif normalizacion == 2:\n",
    "        datosPrueba = normalizarPrueba(datosPrueba,mean,std)\n",
    "    else:\n",
    "        datosPrueba = normalize(datosPrueba, norm=norm,axis=0)\n",
    "    \n",
    "    if eliminacion == 2:\n",
    "        datosPrueba = datosPrueba.drop(['meanEnergy','minEnergy','rangeEnergy','stdEnergy','rangeAmplitude','minMFCC1','stdMFCC1','rangeMFCC2','rangeMFCC4','rangeMFCC12','maxPitch','stdPitch','quantileFormant1','quantileFormant2','rangeFormant2','quantileFormant3','quantileFormant4','rangeFormant4','rangeFormant5','quantileFormant5','stdAmplitude','articulationRate','meanIntensity'], axis=1)\n",
    "    elif eliminacion == 3:\n",
    "        datosPrueba = datosPrueba.drop(['minFormant1','maxFormant3','minHarmonicity','maxFormant5','rangeFormant2','rangeFormant1','maxMFCC6','maxFormant2','minFormant2','maxMFCC4','maxFormant4'], axis=1)\n",
    "    elif eliminacion ==4 :\n",
    "        datosPrueba = datosPrueba.drop(['minFormant1','maxFormant3','minHarmonicity','maxFormant5','rangeFormant1','maxMFCC6','maxFormant2','minFormant2','maxMFCC4','maxFormant4'], axis=1)\n",
    "    return datosPrueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def probar(filename,opcionNormalizacion,opcionEliminacion):\n",
    "    datosPrueba = pd.read_csv(filename, na_values='--undefined--')\n",
    "    nombres = datosPrueba.iloc[:,0]\n",
    "    datosPrueba = datosPrueba.iloc[:,1:]\n",
    "    transformarDatos(datosPrueba,opcionNormalizacion,opcionEliminacion)\n",
    "    datosPrueba = pd.DataFrame(datosPrueba)\n",
    "    resultados = pd.DataFrame(index = nombres)\n",
    "    resultados['RandomForest'] = modelRandom.predict(datosPrueba)\n",
    "    resultados['SVM'] = modelSVC.predict(datosPrueba)\n",
    "    resultados['GradientBoosting'] = modelGBC.predict(datosPrueba)\n",
    "    resultados['NN'] = modelMLP.predict(datosPrueba)\n",
    "    resultados['NB'] = modelNB.predict(datosPrueba)\n",
    "    resultados.replace(0,\"Alegria\",inplace=True)\n",
    "    resultados.replace(1,\"Tristeza\",inplace=True)\n",
    "    resultados.replace(2,\"Miedo\",inplace=True)\n",
    "    resultados.replace(3,\"Asco\",inplace=True)\n",
    "    resultados.replace(4,\"Ira\",inplace=True)\n",
    "    resultados.replace(5,\"Neutral\",inplace=True)\n",
    "    print(resultados)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probar('sonidosPrueba.csv',opN,opE)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
