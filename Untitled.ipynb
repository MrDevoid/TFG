{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.model_selection  import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "import sklearn.feature_selection\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.svm import SVC\n",
    "import itertools\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audiosMezclados = pd.read_csv('sonidosMezclados.csv', na_values='--undefined--')\n",
    "audiosAleman = pd.read_csv('sonidosAleman.csv', na_values='--undefined--')\n",
    "audiosPropios = pd.read_csv('sonidosPropios.csv', na_values='--undefined--')\n",
    "audiosEspanol = pd.read_csv('sonidosEspanol.csv', na_values='--undefined--')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_Mezclados = audiosMezclados.drop(['Result'], axis=1)\n",
    "y_Mezclados = audiosMezclados.Result\n",
    "\n",
    "X_Aleman = audiosAleman.drop(['Result'], axis=1)\n",
    "y_Aleman = audiosAleman.Result\n",
    "\n",
    "X_Propios = audiosPropios.drop(['Result'], axis=1)\n",
    "y_Propios = audiosPropios.Result\n",
    "\n",
    "X_Espanol = audiosEspanol.drop(['Result'], axis=1)\n",
    "y_Espanol = audiosEspanol.Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcularMediaYDesviacion(df):\n",
    "    mean = df.mean(numeric_only=1)\n",
    "    std = df.std(numeric_only=1)\n",
    "    return mean,std\n",
    "def normalizar(df):\n",
    "    mean, std = calcularMediaYDesviacion(df)\n",
    "    df = np.subtract(df,np.expand_dims(mean,axis=0))\n",
    "    df = np.divide(df,np.expand_dims(std,axis=0))\n",
    "    return df, mean, std\n",
    "def normalizarPrueba(df,mean,std):\n",
    "    df = np.subtract(df,np.expand_dims(mean,axis=0))\n",
    "    df = np.divide(df,np.expand_dims(std,axis=0))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deleteNAWithMean(df):\n",
    "    dfMean = df.mean(numeric_only=1) \n",
    "    for i in df.columns.values:\n",
    "        df[i].replace(np.nan, dfMean[i], inplace = True)\n",
    "    return df\n",
    "\n",
    "def deleteNAWithMean2(df,mean):\n",
    "    for i in df.columns.values:\n",
    "        df[i].replace(np.nan, mean[i], inplace = True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_Mezclados = deleteNAWithMean(X_Mezclados)\n",
    "X_Aleman = deleteNAWithMean(X_Aleman)\n",
    "X_Propios = deleteNAWithMean(X_Propios)\n",
    "X_Espanol = deleteNAWithMean(X_Propios)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "correlation = X_Mezclados.corr()\n",
    "limites = [0,5,10,15,21,23,28,33,38,43,48,53,58,63,68,73,78,83,88,94,100,106,112,118]\n",
    "for i in range(0,len(limites)-1):\n",
    "    for j in range(i,len(limites)-1):\n",
    "        correlation = correlation.iloc[limites[i]:limites[i+1],limites[j]:limites[j+1]]\n",
    "        if(np.any(correlation<-0.8) or np.any(correlation>0.8)):\n",
    "            plt.figure()\n",
    "            sns.heatmap(correlation, xticklabels = correlation.columns.values,\n",
    "                        yticklabels=correlation.index.values, vmin=-1,vmax=1,annot=True)\n",
    "            plt.title('Feature correlations')\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(correlation.columns.values[0] + correlation.index.values[0])\n",
    "        correlation = X_Mezclados.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supresion de outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deleteOutliers(data):\n",
    "    lowerOutliers = np.array([False])\n",
    "    upperOutliers = np.array([False])\n",
    "    for i in data.columns.values:\n",
    "        quartiles = data[i].quantile([0.25,0.75])\n",
    "        IQR = quartiles[0.75]-quartiles[0.25]\n",
    "        lowerLimit = quartiles[0.25] - 1.5*IQR\n",
    "        upperLimit = quartiles[0.75] + 1.5*IQR\n",
    "        lowerOutliersAux = data[i].values < lowerLimit\n",
    "        upperOutliersAux = data[i].values > upperLimit\n",
    "        lowerOutliers = np.logical_or(lowerOutliers, lowerOutliersAux)\n",
    "        upperOutliers = np.logical_or(upperOutliers, upperOutliersAux)\n",
    "    upperOutliers = np.logical_and(np.logical_not(lowerOutliers), upperOutliers)\n",
    "    data = data.drop(np.where(lowerOutliers)[0])\n",
    "    data = data.drop(np.where(upperOutliers)[0])\n",
    "    data = data.reset_index(drop=True)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_Mezclados=deleteOutliers(X_MezcladosNor)\n",
    "X_Aleman=deleteOutliers(X_AlemanNor)\n",
    "X_Propios=deleteOutliers(X_PropiosNor)\n",
    "X_Espanol=deleteOutliers(X_EspanolNor)\n",
    "print(len(X_Mezclados))\n",
    "print(len(X_Aleman))\n",
    "print(len(X_Propios))\n",
    "print(len(X_Espanol))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalizar datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "scalerMezclados = scaler.fit(X_Mezclados)\n",
    "scalerAleman = scaler.fit(X_Aleman)\n",
    "scalerPropios = scaler.fit(X_Propios)\n",
    "scalerEspanol = scaler.fit(X_Espanol)\n",
    "minMaxScaled_Mezclados = scalerMezclados.transform(X_Mezclados)\n",
    "minMaxScaled_Aleman = scalerAleman.transform(X_Aleman)\n",
    "minMaxScaled_Propios = scalerPropios.transform(X_Propios)\n",
    "minMaxScaled_Espanol = scalerEspanol.transform(X_Espanol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalizar_Mezclados, meanMezclados, stdMezclados = normalizar(X_Mezclados)\n",
    "normalizar_Aleman, meanAleman, stdAleman = normalizar(X_Aleman)\n",
    "normalizar_Propios, meanPropios, stdPropios = normalizar(X_Propios)\n",
    "normalizar_Espanol, meanEspanol, stdEspanol = normalizar(X_Espanol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_Mezclados, normMezclados = normalize(X_Mezclados, norm = 'l2',axis=0,return_norm=True)\n",
    "normalized_Aleman, normAleman = normalize(X_Aleman, norm='l2',axis=0,return_norm=True)\n",
    "normalized_Propios, normPropios = normalize(X_Propios, norm='l2',axis=0,return_norm=True)\n",
    "normalized_Espanol, normEspanol = normalize(X_Espanol, norm='l2',axis=0,return_norm=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "robustScaler = RobustScaler()\n",
    "robustScalerMezclados = robustScaler.fit(X_Mezclados)\n",
    "robustScalerAleman = robustScaler.fit(X_Aleman)\n",
    "robustScalerPropios = robustScaler.fit(X_Propios)\n",
    "robustScalerEspanol = robustScaler.fit(X_Espanol)\n",
    "robustScaled_Mezclados = robustScalerMezclados.transform(X_Mezclados)\n",
    "robustScaled_Aleman = robustScalerAleman.transform(X_Aleman)\n",
    "robustScaled_Propios = robustScalerPropios.transform(X_Propios)\n",
    "robustScaled_Espanol = robustScalerEspanol.transform(X_Espanol)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seleccionar datos para entrenar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_MezcladosNor = pd.DataFrame(minMaxScaled_Mezclados, columns = X_Mezclados.columns.values)\n",
    "X_AlemanNor = pd.DataFrame(minMaxScaled_Aleman, columns = X_Aleman.columns.values)\n",
    "X_PropiosNor = pd.DataFrame(minMaxScaled_Propios, columns = X_Propios.columns.values)\n",
    "X_EspanolNor = pd.DataFrame(minMaxScaled_Espanol, columns = X_Espanol.columns.values)\n",
    "\n",
    "#X_MezcladosNor = pd.DataFrame(normalizar_Mezclados)\n",
    "#X_AlemanNor = pd.DataFrame(normalizar_Aleman)\n",
    "#X_PropiosNor = pd.DataFrame(normalizar_Propios)\n",
    "#X_EspanolNor = pd.DataFrame(normaliar_Espanol)\n",
    "\n",
    "#X_MezcladosNor = pd.DataFrame(normalized_Mezclados)\n",
    "#X_AlemanNor = pd.DataFrame(normalized_Aleman)\n",
    "#X_PropiosNor = pd.DataFrame(normalized_Propios)\n",
    "#X_EspanolNor = pd.DataFrame(normalizaed_Espanol)\n",
    "\n",
    "#X_MezcladosNor = pd.DataFrame(robustScaled_Mezclados)\n",
    "#X_AlemanNor = pd.DataFrame(robustScaled_Aleman)\n",
    "#X_PropiosNor = pd.DataFrame(robustScaled_Propios)\n",
    "#X_EspanolNor = pd.DataFrame(robustScaled_Espanol)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seleccion de variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model = ExtraTreesClassifier(n_estimators=90)\n",
    "model = model.fit(X_MezcladosNor,y_Mezclados)\n",
    "featureImportance = model.feature_importances_\n",
    "print(featureImportance)\n",
    "featureImportance = np.argsort(featureImportance)\n",
    "print(featureImportance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def moreImportantFeatures(how_many, classifier, inputData, outputData):\n",
    "    model = classifier.fit(inputData,outputData)\n",
    "    featureImportance = model.feature_importances_\n",
    "    orderImportance = np.argsort(featureImportance)\n",
    "    if(how_many >=0):\n",
    "        for i in range(1,how_many):\n",
    "            print(inputData.columns.values[orderImportance[-i]])\n",
    "    else:\n",
    "        for i in range(1,-how_many):\n",
    "            print(inputData.columns.values[orderImportance[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moreImportantFeatures(-20, ExtraTreesClassifier(n_estimators=90),X_AlemanNor,y_Aleman)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_MezcladosNor80 = X_MezcladosNor.drop(['meanEnergy','minEnergy','rangeEnergy','stdEnergy','rangeAmplitude','minMFCC1','stdMFCC1','rangeMFCC2','rangeMFCC4','rangeMFCC12','maxPitch','stdPitch','quantileFormant1','quantileFormant2','rangeFormant2','quantileFormant3','quantileFormant4','rangeFormant4','rangeFormant5','quantileFormant5','stdAmplitude','articulationRate','meanIntensity'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_MezcladosNorLessImportant = X_MezcladosNor.drop(['minFormant1','maxFormant3','minHarmonicity','maxFormant5','rangeFormant2','rangeFormant1','maxMFCC6','maxFormant2','minFormant2','maxMFCC4','maxFormant4'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_MezcladosNor80LessImportant = X_MezcladosNor80.drop(['minFormant1','maxFormant3','minHarmonicity','maxFormant5','rangeFormant2','rangeFormant1','maxMFCC6','maxFormant2','minFormant2','maxMFCC4','maxFormant4'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def histogramas(data):\n",
    "    for nameOfColumn in data.columns.values:\n",
    "        data.hist(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "histogramas(X_Mezclados)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pruebas para determinar valores adecuados para cada algoritmo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bestNumberOfEstimatorsAda = 0\n",
    "maxValueABC = 0.0\n",
    "for i in range(1, 101, 10):\n",
    "    model = AdaBoostClassifier(n_estimators=i)\n",
    "    scores = cross_val_score(model, X_MezcladosNor, y_Mezclados, cv=10, scoring='accuracy')\n",
    "    if scores.mean() > maxValueABC:\n",
    "        maxValueABC = scores.mean()\n",
    "        bestNumberOfEstimatorsAda = i\n",
    "    print('Numero estimadores', i)\n",
    "    print(scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bestNumberOfEstimatorsGBC = 0\n",
    "maxValueGBC = 0.0\n",
    "for i in range(1, 101, 10):\n",
    "    model = GradientBoostingClassifier(random_state=1, n_estimators=i)\n",
    "    scores = cross_val_score(model, X_MezcladosNor, y_Mezclados, cv=10, scoring='accuracy')\n",
    "    if scores.mean() > maxValueGBC:\n",
    "        maxValueGBC = scores.mean()\n",
    "        bestNumberOfEstimatorsGBC = i\n",
    "    print('Numero estimadores' , i)\n",
    "    print(scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bestNumberOfEstimatorsRFC = 0\n",
    "maxValueRFC = 0\n",
    "for i in range(1, 101, 10):\n",
    "    model = RandomForestClassifier(n_estimators=i)\n",
    "    scores = cross_val_score(model, X_MezcladosNor, y_Mezclados, cv=10, scoring='accuracy')\n",
    "    if scores.mean() > maxValueRFC:\n",
    "        maxValueRFC = scores.mean()\n",
    "        bestNumberOfEstimatorsRFC = i\n",
    "    print('Numero estimadores' , i)\n",
    "    print(scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mejorC = 0\n",
    "mejorGamma = 0\n",
    "maxPredictSVM = 0.0\n",
    "for i in [1, 10, 100, 1000]:\n",
    "    for j in [0.1, 0.01, 0.001, 1]:\n",
    "        model = SVC(kernel='rbf', C=i, gamma=j)\n",
    "        scores = cross_val_score(model, X_MezcladosNor, y_Mezclados, cv=10, scoring='accuracy')\n",
    "        if scores.mean() > maxPredictSVM:\n",
    "            maxPredictSVM = scores.mean()\n",
    "            mejorC = i\n",
    "            mejorGamma = j\n",
    "        print('C =', i, 'gamma =', j, ':', scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creacion y entrenamiento de algoritmos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelAda = AdaBoostClassifier(n_estimators=bestNumberOfEstimatorsAda)\n",
    "modelRandom = RandomForestClassifier(n_estimators=bestNumberOfEstimatorsRFC)\n",
    "modelSVC = SVC(kernel='rbf', C=mejorC, gamma=mejorGamma)\n",
    "modelGBC = GradientBoostingClassifier(random_state=1, n_estimators=bestNumberOfEstimatorsGBC)\n",
    "modelAda.fit(X_MezcladosNor,y_Mezclados)\n",
    "modelRandom.fit(X_MezcladosNor,y_Mezclados)\n",
    "modelSVC.fit(X_MezcladosNor,y_Mezclados)\n",
    "modelGBC.fit(X_MezcladosNor,y_Mezclados)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pruebas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_MezcladosNor, y_Mezclados, random_state=0)\n",
    "class_names = ['Alegría', 'Tristeza', 'Miedo','Asco','Ira','Neutral']\n",
    "\n",
    "#y_pred = modelSVC.fit(X_train, y_train).predict(X_test)\n",
    "y_pred = modelRandom.fit(X_train, y_train).predict(X_test)\n",
    "#y_pred = modelGBC.fit(X_train, y_train).predict(X_test)\n",
    "#\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(cm, classes, \n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()\n",
    "\n",
    "\n",
    "# Compute confusion matrix\n",
    "cnf_matrix = confusion_matrix(y_test, y_pred)\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=class_names,\n",
    "                      title='Confusion matrix, without normalization')\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=class_names, normalize=True,\n",
    "                      title='Normalized confusion matrix')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def probar(filename):\n",
    "    datosPrueba = pd.read_csv(filename, na_values='--undefined--')\n",
    "    nombres = datosPrueba.iloc[:,0]\n",
    "    datosPrueba = datosPrueba.iloc[:,1:]\n",
    "    datosPrueba = deleteNAWithMean2(datosPrueba,X_Mezclados.mean(numeric_only=1))\n",
    "    datosPrueba = scalerMezclados.transform(datosPrueba)\n",
    "    datosPrueba = pd.DataFrame(datosPrueba)\n",
    "    resultados = pd.DataFrame(index = nombres)\n",
    "    resultados['AdaBoost'] = modelAda.predict(datosPrueba)\n",
    "    resultados['RandomForest'] = modelRandom.predict(datosPrueba)\n",
    "    resultados['SVM'] = modelSVC.predict(datosPrueba)\n",
    "    resultados['GradientBoosting'] = modelGBC.predict(datosPrueba)\n",
    "    \n",
    "    resultados.replace(0,\"Alegria\",inplace=True)\n",
    "    resultados.replace(1,\"Tristeza\",inplace=True)\n",
    "    resultados.replace(2,\"Miedo\",inplace=True)\n",
    "    resultados.replace(3,\"Asco\",inplace=True)\n",
    "    resultados.replace(4,\"Ira\",inplace=True)\n",
    "    resultados.replace(5,\"Neutral\",inplace=True)\n",
    "    print(resultados)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probar('sonidosPrueba.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
