{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trabajo de fin de grado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se importan las librerias necesarias, en primer lugar algunas librerias para tratar con los tipos de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se importan tambien los algoritmos que se utilizaran más adelante para predecir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jose/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.model_selection  import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.naive_bayes import GaussianNB \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Librerias para el preprocesado y analisis de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Utilizados para normalizar\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "#Para la seleccion de variables\n",
    "import sklearn.feature_selection\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y otras de ayuda para plotear gráficas y mostrar métricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import itertools\n",
    "from sklearn.metrics import confusion_matrix, precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se comienza con la carga de los archivos .csv que han de encontrarse en el mismo directorio. Como se ve en el código cuando el valor no está definido aparece *--undefined--* y por ello esos valores son tratados como NA's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "audiosMezclados = pd.read_csv('sonidosMezclados.csv', na_values='--undefined--')\n",
    "audiosAleman = pd.read_csv('sonidosAleman.csv', na_values='--undefined--')\n",
    "audiosPropios = pd.read_csv('sonidosPropios.csv', na_values='--undefined--')\n",
    "audiosEspanol = pd.read_csv('sonidosEspanol.csv', na_values='--undefined--')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observando los cantidad de audios de las diferentes clases que se encuentran en cada uno de los dataframes, es fácil ver que en dataframe de audios en Español hay una presencia muy elevada de audios neutrales, por ello a continuación se eliminan varios de ellos de forma que finalmente todos estén presentes en la misma proporción"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "neutrales = audiosEspanol[audiosEspanol['Result']==5]\n",
    "audiosEspanol = audiosEspanol.drop(np.arange((neutrales.index[0]+184),neutrales.index[-1]+1))\n",
    "audiosEspanol = audiosEspanol.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación se separan los dataframes en dos, uno con los datos de entrenamiento y otro con la etiqueta de cada ejemplo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_Mezclados = audiosMezclados.drop(['Result'], axis=1)\n",
    "y_Mezclados = audiosMezclados.Result\n",
    "\n",
    "X_Aleman = audiosAleman.drop(['Result'], axis=1)\n",
    "y_Aleman = audiosAleman.Result\n",
    "\n",
    "X_Propios = audiosPropios.drop(['Result'], axis=1)\n",
    "y_Propios = audiosPropios.Result\n",
    "\n",
    "X_Espanol = audiosEspanol.drop(['Result'], axis=1)\n",
    "y_Espanol = audiosEspanol.Result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se definen unas funciones utiles para normalizar los datos de entrenamiento y las pruebas que se realizan más adelante; junto con otras para eliminar valores faltantes con la media."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcularMediaYDesviacion(df):\n",
    "    mean = df.mean(numeric_only=1)\n",
    "    std = df.std(numeric_only=1)\n",
    "    return mean,std\n",
    "def normalizar(df, mean, std):\n",
    "    df = np.subtract(df,np.expand_dims(mean,axis=0))\n",
    "    df = np.divide(df,np.expand_dims(std,axis=0))\n",
    "    return df\n",
    "def normalizarPrueba(df,mean,std):\n",
    "    df = np.subtract(df,np.expand_dims(mean,axis=0))\n",
    "    df = np.divide(df,np.expand_dims(std,axis=0))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deleteNAWithMean(df,mean):\n",
    "    for i in df.columns.values:\n",
    "        df[i].replace(np.nan, mean[i], inplace = True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dado que existen varios valores nulos, se reemplazan con la media de la columna a la que pertenecen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "meanMezclados, stdMezclados = calcularMediaYDesviacion(X_Mezclados)\n",
    "meanAleman, stdAleman = calcularMediaYDesviacion(X_Aleman)\n",
    "meanPropios, stdPropios = calcularMediaYDesviacion(X_Propios)\n",
    "meanEspanol, stdEspanol = calcularMediaYDesviacion(X_Espanol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_Mezclados = deleteNAWithMean(X_Mezclados,meanMezclados)\n",
    "X_Aleman = deleteNAWithMean(X_Aleman,meanAleman)\n",
    "X_Propios = deleteNAWithMean(X_Propios,meanPropios)\n",
    "X_Espanol = deleteNAWithMean(X_Espanol,meanEspanol)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para realizar un analisis sobre la correlación que existe entre las variables, se muestran algunos mapas de calor en los que existe al menos un par de variables con una correlación absoluta mayor a 0.8, para después valorar la eliminación de alguna de esas variables con la finalidad de reducir los datos de entrenamiento necesarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "correlation = X_Mezclados.corr()\n",
    "limites = [0,5,10,15,21,23,28,33,38,43,48,53,58,63,68,73,78,83,88,94,100,106,112,118]\n",
    "for i in range(0,len(limites)-1):\n",
    "    for j in range(i,len(limites)-1):\n",
    "        correlation = correlation.iloc[limites[i]:limites[i+1],limites[j]:limites[j+1]]\n",
    "        if(np.any(correlation<-0.8) or np.any(correlation>0.8)):\n",
    "            plt.figure()\n",
    "            sns.heatmap(correlation, xticklabels = correlation.columns.values,\n",
    "                        yticklabels=correlation.index.values, vmin=-1,vmax=1,annot=True)\n",
    "            plt.title('Feature correlations')\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(correlation.columns.values[0] + correlation.index.values[0])\n",
    "        correlation = X_Mezclados.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalizar datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se definen varios métodos para normalizar con el objetivo de poder comparar los resultados que producen cada una de ellas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalización utilizando el método MinMax, que convierte los valores a un intervalo de 0-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "scalerMezclados = scaler.fit(X_Mezclados)\n",
    "scalerAleman = scaler.fit(X_Aleman)\n",
    "scalerPropios = scaler.fit(X_Propios)\n",
    "scalerEspanol = scaler.fit(X_Espanol)\n",
    "minMaxScaled_Mezclados = scalerMezclados.transform(X_Mezclados)\n",
    "minMaxScaled_Aleman = scalerAleman.transform(X_Aleman)\n",
    "minMaxScaled_Propios = scalerPropios.transform(X_Propios)\n",
    "minMaxScaled_Espanol = scalerEspanol.transform(X_Espanol)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalización ZScore, para transformar los datos a una normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalizar_Mezclados = normalizar(X_Mezclados,meanMezclados,stdMezclados)\n",
    "normalizar_Aleman = normalizar(X_Aleman,meanAleman,stdAleman)\n",
    "normalizar_Propios = normalizar(X_Propios,meanPropios,stdPropios)\n",
    "normalizar_Espanol = normalizar(X_Espanol,meanEspanol,stdEspanol)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalización utilizando la norma l1, l2 o max, según se especifique en los argumentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_Mezclados, normMezclados = normalize(X_Mezclados, norm = 'l2',axis=0,return_norm=True)\n",
    "normalized_Aleman, normAleman = normalize(X_Aleman, norm='l2',axis=0,return_norm=True)\n",
    "normalized_Propios, normPropios = normalize(X_Propios, norm='l2',axis=0,return_norm=True)\n",
    "normalized_Espanol, normEspanol = normalize(X_Espanol, norm='l2',axis=0,return_norm=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalización realizada con el rango interquartílico, de forma que sea más robusta frente a valores atípicos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "robustScaler = RobustScaler()\n",
    "robustScalerMezclados = robustScaler.fit(X_Mezclados)\n",
    "robustScalerAleman = robustScaler.fit(X_Aleman)\n",
    "robustScalerPropios = robustScaler.fit(X_Propios)\n",
    "robustScalerEspanol = robustScaler.fit(X_Espanol)\n",
    "robustScaled_Mezclados = robustScalerMezclados.transform(X_Mezclados)\n",
    "robustScaled_Aleman = robustScalerAleman.transform(X_Aleman)\n",
    "robustScaled_Propios = robustScalerPropios.transform(X_Propios)\n",
    "robustScaled_Espanol = robustScalerEspanol.transform(X_Espanol)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seleccionar datos para entrenar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para facilitar la selección de los datos se ponen a disposicion asignaciones de todas las normalizaciones de los datos, de forma que bastará con descomentar las asignaciones que se quieran realizar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#opN=1\n",
    "#X_MezcladosNor, scalerM = pd.DataFrame(minMaxScaled_Mezclados, columns = X_Mezclados.columns.values), scalerMezclados\n",
    "#X_AlemanNor, scalerA = pd.DataFrame(minMaxScaled_Aleman, columns = X_Aleman.columns.values), scalerAleman\n",
    "#X_PropiosNor, scalerP = pd.DataFrame(minMaxScaled_Propios, columns = X_Propios.columns.values), scalerPropios\n",
    "#X_EspanolNor, scalerE = pd.DataFrame(minMaxScaled_Espanol, columns = X_Espanol.columns.values), scalerEspanol\n",
    "\n",
    "opN=2\n",
    "X_MezcladosNor = pd.DataFrame(normalizar_Mezclados)\n",
    "X_AlemanNor = pd.DataFrame(normalizar_Aleman)\n",
    "X_PropiosNor = pd.DataFrame(normalizar_Propios)\n",
    "X_EspanolNor = pd.DataFrame(normalizar_Espanol)\n",
    "\n",
    "#opN=3\n",
    "#X_MezcladosNor = pd.DataFrame(normalized_Mezclados, columns = X_Mezclados.columns.values)\n",
    "#X_AlemanNor = pd.DataFrame(normalized_Aleman, columns = X_Aleman.columns.values)\n",
    "#X_PropiosNor = pd.DataFrame(normalized_Propios, columns = X_Propios.columns.values)\n",
    "#X_EspanolNor = pd.DataFrame(normalized_Espanol, columns = X_Espanol.columns.values)\n",
    "\n",
    "#opN=4\n",
    "#X_MezcladosNor, scalerM = pd.DataFrame(robustScaled_Mezclados, columns = X_Mezclados.columns.values), robustScalerMezclados\n",
    "#X_AlemanNor, scalerA = pd.DataFrame(robustScaled_Aleman, columns = X_Aleman.columns.values), robustScalerAleman\n",
    "#X_PropiosNor, scalerP = pd.DataFrame(robustScaled_Propios, columns = X_Propios.columns.values), robustScalerPropios\n",
    "#X_EspanolNor, scalerE = pd.DataFrame(robustScaled_Espanol, columns = X_Espanol.columns.values), robustScalerEspanol"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seleccion de variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para seleccionar variables a eliminar, a parte de utilizar el analisis que se hizo anteriormente en base a las correlaciones; se lanza un algoritmo de decisión para que establezca un orden en la importancia de las variables. Así las variables que el arbol considere menos importantes se podrán eliminar, al igual que las que considere más importantes se procurará mantenerlas "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.01615075 0.01137402 0.00817117 0.01258081 0.00851799 0.02983802\n",
      " 0.01641804 0.01446827 0.01331517 0.01196698 0.01051351 0.0037027\n",
      " 0.00381404 0.00836288 0.00379323 0.01984727 0.00994736 0.00789272\n",
      " 0.00904163 0.00750175 0.01517228 0.00812157 0.00786965 0.0074955\n",
      " 0.00671722 0.01536059 0.00551614 0.00761026 0.01083297 0.00711312\n",
      " 0.00867214 0.00860216 0.00556545 0.01656703 0.0147768  0.01489909\n",
      " 0.0056444  0.00840655 0.00878241 0.00838313 0.00846133 0.00650192\n",
      " 0.00584973 0.01439406 0.01050084 0.0054726  0.00532532 0.00411507\n",
      " 0.01357661 0.01073013 0.00806629 0.00451948 0.00468239 0.0100874\n",
      " 0.00716666 0.00674588 0.00428096 0.00388177 0.01341399 0.02320087\n",
      " 0.00674587 0.0049632  0.00665431 0.0070868  0.00584731 0.00513602\n",
      " 0.00509529 0.0040984  0.00988789 0.00714714 0.00619234 0.00452598\n",
      " 0.00446614 0.01984981 0.0087996  0.00931039 0.01081433 0.00534211\n",
      " 0.00847785 0.00678025 0.00515927 0.00609941 0.00521518 0.01078011\n",
      " 0.00784419 0.00547633 0.00699081 0.00358142 0.00245711 0.00406699\n",
      " 0.00829307 0.00631396 0.00599504 0.00263562 0.00495577 0.00360955\n",
      " 0.00689787 0.00540893 0.00545346 0.00487271 0.00559314 0.00489529\n",
      " 0.0074424  0.00940834 0.01501622 0.00501836 0.00502315 0.00348434\n",
      " 0.00923745 0.00962543 0.00550458 0.00478105 0.00817181 0.00383625\n",
      " 0.01539069 0.01663038 0.01427801 0.00703519]\n",
      "[ 88  93 107  87  95  11  14  12 113  57  89  67  47  56  72  51  71  52\n",
      " 111  99 101  94  61 105 106  66  65  80  82  46  77  97  98  45  85 110\n",
      "  26  32 100  36  64  42  92  81  70  91  41  62  24  60  55  79  96  86\n",
      " 117  63  29  69  54 102  23  19  27  84  22  17  50  21   2 112  90  13\n",
      "  39  37  40  78   4  31  30  38  74  18 108  75 103 109  68  16  53  44\n",
      "  10  49  83  76  28   1   9   3   8  58  48 116  43   7  34  35 104  20\n",
      "  25 114   0   6  33 115  15  73  59   5]\n"
     ]
    }
   ],
   "source": [
    "model = ExtraTreesClassifier(n_estimators=90)\n",
    "model = model.fit(X_MezcladosNor,y_Mezclados)\n",
    "featureImportance = model.feature_importances_\n",
    "print(featureImportance)\n",
    "featureImportance = np.argsort(featureImportance)\n",
    "print(featureImportance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tras entrenar con el arbol, se implementa un metodo que devuelve las variables más o menos importantes, en función del signo del parámetro de entrada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def moreImportantFeatures(how_many, classifier, inputData, outputData):\n",
    "    model = classifier.fit(inputData,outputData)\n",
    "    featureImportance = model.feature_importances_\n",
    "    orderImportance = np.argsort(featureImportance)\n",
    "    if(how_many >=0):\n",
    "        for i in range(1,how_many):\n",
    "            print(inputData.columns.values[orderImportance[-i]])\n",
    "    else:\n",
    "        for i in range(1,-how_many):\n",
    "            print(inputData.columns.values[orderImportance[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moreImportantFeatures(-20, ExtraTreesClassifier(n_estimators=90),X_AlemanNor,y_Aleman)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación se crean varios conjuntos eliminando algunas variables. En uno de ellos eliminamos variables que tenian más de un 0.8 de correlación absoluta; otro eliminando aquellas variables que el arbol de decisión determina que son menos influyentes y un ultimo eliminando variables que cumplan alguna de las condiciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_MezcladosNor80 = X_MezcladosNor.drop(['meanEnergy','minEnergy','rangeEnergy','stdEnergy','rangeAmplitude','minMFCC1','stdMFCC1','rangeMFCC2','rangeMFCC4','rangeMFCC12','maxPitch','stdPitch','quantileFormant1','quantileFormant2','rangeFormant2','quantileFormant3','quantileFormant4','rangeFormant4','rangeFormant5','quantileFormant5','stdAmplitude','articulationRate','meanIntensity'], axis=1)\n",
    "X_AlemanNor80 = X_AlemanNor.drop(['meanEnergy','minEnergy','rangeEnergy','stdEnergy','rangeAmplitude','minMFCC1','stdMFCC1','rangeMFCC2','rangeMFCC4','rangeMFCC12','maxPitch','stdPitch','quantileFormant1','quantileFormant2','rangeFormant2','quantileFormant3','quantileFormant4','rangeFormant4','rangeFormant5','quantileFormant5','stdAmplitude','articulationRate','meanIntensity'], axis=1)\n",
    "X_EspanolNor80 = X_EspanolNor.drop(['meanEnergy','minEnergy','rangeEnergy','stdEnergy','rangeAmplitude','minMFCC1','stdMFCC1','rangeMFCC2','rangeMFCC4','rangeMFCC12','maxPitch','stdPitch','quantileFormant1','quantileFormant2','rangeFormant2','quantileFormant3','quantileFormant4','rangeFormant4','rangeFormant5','quantileFormant5','stdAmplitude','articulationRate','meanIntensity'], axis=1)\n",
    "X_PropiosNor80 = X_PropiosNor.drop(['meanEnergy','minEnergy','rangeEnergy','stdEnergy','rangeAmplitude','minMFCC1','stdMFCC1','rangeMFCC2','rangeMFCC4','rangeMFCC12','maxPitch','stdPitch','quantileFormant1','quantileFormant2','rangeFormant2','quantileFormant3','quantileFormant4','rangeFormant4','rangeFormant5','quantileFormant5','stdAmplitude','articulationRate','meanIntensity'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_MezcladosNorLessImportant = X_MezcladosNor.drop(['minFormant1','maxFormant3','minHarmonicity','maxFormant5','rangeFormant2','rangeFormant1','maxMFCC6','maxFormant2','minFormant2','maxMFCC4','maxFormant4'], axis=1)\n",
    "X_AlemanNorLessImportant = X_AlemanNor.drop(['minFormant1','maxFormant3','minHarmonicity','maxFormant5','rangeFormant2','rangeFormant1','maxMFCC6','maxFormant2','minFormant2','maxMFCC4','maxFormant4'], axis=1)\n",
    "X_EspanolNorLessImportant = X_EspanolNor.drop(['minFormant1','maxFormant3','minHarmonicity','maxFormant5','rangeFormant2','rangeFormant1','maxMFCC6','maxFormant2','minFormant2','maxMFCC4','maxFormant4'], axis=1)\n",
    "X_PropiosNorLessImportant = X_PropiosNor.drop(['minFormant1','maxFormant3','minHarmonicity','maxFormant5','rangeFormant2','rangeFormant1','maxMFCC6','maxFormant2','minFormant2','maxMFCC4','maxFormant4'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_MezcladosNor80LessImportant = X_MezcladosNor80.drop(['minFormant1','maxFormant3','minHarmonicity','maxFormant5','rangeFormant1','maxMFCC6','maxFormant2','minFormant2','maxMFCC4','maxFormant4'], axis=1)\n",
    "X_AlemanNor80LessImportant = X_AlemanNor80.drop(['minFormant1','maxFormant3','minHarmonicity','maxFormant5','rangeFormant1','maxMFCC6','maxFormant2','minFormant2','maxMFCC4','maxFormant4'], axis=1)\n",
    "X_EspanolNor80LessImportant = X_EspanolNor80.drop(['minFormant1','maxFormant3','minHarmonicity','maxFormant5','rangeFormant1','maxMFCC6','maxFormant2','minFormant2','maxMFCC4','maxFormant4'], axis=1)\n",
    "X_PropiosNor80LessImportant = X_PropiosNor80.drop(['minFormant1','maxFormant3','minHarmonicity','maxFormant5','rangeFormant1','maxMFCC6','maxFormant2','minFormant2','maxMFCC4','maxFormant4'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def histogramas(data):\n",
    "    for nameOfColumn in data.columns.values:\n",
    "        data.hist(nameOfColumn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "histogramas(X_Mezclados)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nuevamente tras finalizar esta fase se dispone una serie de opciones para determinar con que conjunto se procedera a la eliminación de outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#opE =1\n",
    "#X_MezcladosReduced = X_MezcladosNor\n",
    "#X_AlemanReduced = X_AlemanNor\n",
    "#X_PropiosReduced = X_PropiosNor\n",
    "#X_EspanolReduced = X_EspanolNor\n",
    "\n",
    "#opE=2\n",
    "#X_MezcladosReduced = X_MezcladosNor80\n",
    "#X_AlemanReduced = X_AlemanNor80\n",
    "#X_PropiosReduced = X_PropiosNor80\n",
    "#X_EspanolReduced = X_EspanolNor80\n",
    "\n",
    "#opE=3\n",
    "#X_MezcladosReduced = X_MezcladosNorLessImportant\n",
    "#X_AlemanReduced = X_AlemanNorLessImportant\n",
    "#X_PropiosReduced = X_PropiosNorLessImportant\n",
    "#X_EspanolReduced = X_EspanolNorLessImportant\n",
    "\n",
    "opE=4\n",
    "X_MezcladosReduced = X_MezcladosNor80LessImportant\n",
    "X_AlemanReduced = X_AlemanNor80LessImportant\n",
    "X_PropiosReduced = X_PropiosNor80LessImportant\n",
    "X_EspanolReduced = X_EspanolNor80LessImportant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supresion de outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para eliminar los outliers se define una función que calcula el primer y tercer cuartil y con ello el rango intercuartílico. En este caso, dado que el porcentaje de outliers inicial era demasiado elevado, el valor que multiplica al rango intercualtílico ha sido aumentado para ser más flexibles a la hora de determinar si un valor es atípico."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deleteOutliers(data,outdata):\n",
    "    lowerOutliers = np.array([False])\n",
    "    upperOutliers = np.array([False])\n",
    "    for i in data.columns.values:\n",
    "        quartiles = data[i].quantile([0.25,0.75])\n",
    "        IQR = quartiles[0.75]-quartiles[0.25]\n",
    "        lowerLimit = quartiles[0.25] - 2.5*IQR\n",
    "        upperLimit = quartiles[0.75] + 2.5*IQR\n",
    "        lowerOutliersAux = data[i].values < lowerLimit\n",
    "        upperOutliersAux = data[i].values > upperLimit\n",
    "        lowerOutliers = np.logical_or(lowerOutliers, lowerOutliersAux)\n",
    "        upperOutliers = np.logical_or(upperOutliers, upperOutliersAux)\n",
    "    outliers = np.logical_or(lowerOutliers, upperOutliers)\n",
    "    data = data.drop(np.where(outliers)[0])\n",
    "    outdata = outdata.drop(np.where(outliers)[0])\n",
    "    data = data.reset_index(drop=True)\n",
    "    outdata = outdata.reset_index(drop=True)\n",
    "    return data, outdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_MezcladosNorWithoutOutliers,y_MezcladosWithoutOutliers=deleteOutliers(X_MezcladosReduced,y_Mezclados)\n",
    "X_AlemanNorWithoutOutliers, y_AlemanWithoutOutliers=deleteOutliers(X_AlemanReduced, y_Aleman)\n",
    "X_PropiosNorWithoutOutliers, y_PropiosWithoutOutliers=deleteOutliers(X_PropiosReduced, y_Propios)\n",
    "X_EspanolNorWithoutOutliers, y_EspanolWithoutOutliers=deleteOutliers(X_EspanolReduced, y_Espanol)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente se decide con que conjunto de datos se procede a entrenar y validar los algoritmos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_Train, y_Train, mean, std = X_MezcladosReduced,y_Mezclados, meanMezclados, stdMezclados\n",
    "#X_Train, y_Train, mean, std = X_AlemanReduced,y_Aleman, meanAleman, stdAleman\n",
    "#X_Train, y_Train, mean, std = X_PropiosReduced,y_Propios, meanPropios, stdPropios\n",
    "#X_Train, y_Train, mean, std = X_EspanolReduced,y_Espanol, meanEspanol, stdEspanol\n",
    "#X_Train, y_Train, mean, std = X_MezcladosNorWithoutOutliers,y_MezcladosWithoutOutliers, meanMezclados, stdMezclados\n",
    "#X_Train, y_Train, mean, std = X_AlemanNorWithoutOutliers,y_AlemanWithoutOutliers, meanAleman, stdAleman\n",
    "#X_Train, y_Train, mean, std = X_PropiosNorWithoutOutliers,y_PropiosWithoutOutliers, meanPropios, stdPropios\n",
    "X_Train, y_Train, mean, std = X_EspanolNorWithoutOutliers,y_EspanolWithoutOutliers, meanEspanol, stdEspanol\n",
    "\n",
    "#En caso de utilizar un metodo de normalización que se realice mediante un scaler\n",
    "#scaler = scalerM\n",
    "#scaler = scalerA\n",
    "#scaler = scalerP\n",
    "#scaler = scalerE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pruebas para determinar valores adecuados para cada algoritmo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numero estimadores 1\n",
      "0.3535840976966181\n",
      "Tiempo estimado: 0.12768864631652832 segundos\n",
      "Numero estimadores 11\n",
      "0.37694066650676994\n",
      "Tiempo estimado: 0.8125002384185791 segundos\n",
      "Numero estimadores 21\n",
      "0.37360277320991625\n",
      "Tiempo estimado: 1.43221116065979 segundos\n",
      "Numero estimadores 31\n",
      "0.37943040658884924\n",
      "Tiempo estimado: 2.0399298667907715 segundos\n",
      "Numero estimadores 41\n",
      "0.38870537238912284\n",
      "Tiempo estimado: 2.6850168704986572 segundos\n",
      "Numero estimadores 51\n",
      "0.3712498320334457\n",
      "Tiempo estimado: 3.336374282836914 segundos\n",
      "Numero estimadores 61\n",
      "0.37590099482414335\n",
      "Tiempo estimado: 3.980569362640381 segundos\n",
      "Numero estimadores 71\n",
      "0.38752890180088756\n",
      "Tiempo estimado: 4.677144765853882 segundos\n",
      "Numero estimadores 81\n",
      "0.3712498320334457\n",
      "Tiempo estimado: 5.319269418716431 segundos\n",
      "Numero estimadores 91\n",
      "0.37590099482414335\n",
      "Tiempo estimado: 5.902386426925659 segundos\n"
     ]
    }
   ],
   "source": [
    "bestNumberOfEstimatorsAda = 0\n",
    "maxValueABC = 0.0\n",
    "for i in range(1, 101, 10):\n",
    "    ini = time.time()\n",
    "    model = AdaBoostClassifier(n_estimators=i)\n",
    "    scores = cross_val_score(model, X_Train, y_Train, cv=10, scoring='accuracy')\n",
    "    fin = time.time()\n",
    "    if scores.mean() > maxValueABC:\n",
    "        maxValueABC = scores.mean()\n",
    "        bestNumberOfEstimatorsAda = i\n",
    "    print('Numero estimadores', i)\n",
    "    print(scores.mean())\n",
    "    print('Tiempo estimado:', fin-ini, 'segundos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numero estimadores 1\n",
      "0.78842419211081\n",
      "Tiempo estimado: 0.3358948230743408 segundos\n",
      "Numero estimadores 11\n",
      "0.8730518833743307\n",
      "Tiempo estimado: 3.0444140434265137 segundos\n",
      "Numero estimadores 21\n",
      "0.8914724885486743\n",
      "Tiempo estimado: 5.748614072799683 segundos\n",
      "Numero estimadores 31\n",
      "0.8948194225135859\n",
      "Tiempo estimado: 8.470807790756226 segundos\n",
      "Numero estimadores 41\n",
      "0.908472068534928\n",
      "Tiempo estimado: 11.575325012207031 segundos\n",
      "Numero estimadores 51\n",
      "0.9119203336760796\n",
      "Tiempo estimado: 14.259109020233154 segundos\n",
      "Numero estimadores 61\n",
      "0.9153439952755509\n",
      "Tiempo estimado: 16.56233859062195 segundos\n",
      "Numero estimadores 71\n",
      "0.9153850139872377\n",
      "Tiempo estimado: 19.576837062835693 segundos\n",
      "Numero estimadores 81\n",
      "0.9164675907811688\n",
      "Tiempo estimado: 23.66685914993286 segundos\n",
      "Numero estimadores 91\n",
      "0.9211324334624272\n",
      "Tiempo estimado: 25.71035647392273 segundos\n"
     ]
    }
   ],
   "source": [
    "bestNumberOfEstimatorsGBC = 0\n",
    "maxValueGBC = 0.0\n",
    "for i in range(1, 101, 10):\n",
    "    ini = time.time()\n",
    "    model = GradientBoostingClassifier(random_state=1, n_estimators=i)\n",
    "    scores = cross_val_score(model, X_Train, y_Train, cv=10, scoring='accuracy')\n",
    "    fin = time.time()\n",
    "    if scores.mean() > maxValueGBC:\n",
    "        maxValueGBC = scores.mean()\n",
    "        bestNumberOfEstimatorsGBC = i\n",
    "    print('Numero estimadores' , i)\n",
    "    print(scores.mean())\n",
    "    print('Tiempo estimado:', fin-ini, 'segundos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numero estimadores 1\n",
      "0.6504925140253086\n",
      "Tiempo estimado: 0.06853675842285156 segundos\n",
      "Numero estimadores 11\n",
      "0.8856191861232221\n",
      "Tiempo estimado: 0.3623356819152832 segundos\n",
      "Numero estimadores 21\n",
      "0.9110117215560896\n",
      "Tiempo estimado: 0.6710004806518555 segundos\n",
      "Numero estimadores 31\n",
      "0.9108126049710824\n",
      "Tiempo estimado: 0.971827507019043 segundos\n",
      "Numero estimadores 41\n",
      "0.9178528544054769\n",
      "Tiempo estimado: 1.2580435276031494 segundos\n",
      "Numero estimadores 51\n",
      "0.921279687793889\n",
      "Tiempo estimado: 1.5620005130767822 segundos\n",
      "Numero estimadores 61\n",
      "0.9268711581165489\n",
      "Tiempo estimado: 1.8558225631713867 segundos\n",
      "Numero estimadores 71\n",
      "0.9233972868088653\n",
      "Tiempo estimado: 2.174384832382202 segundos\n",
      "Numero estimadores 81\n",
      "0.9314549659992855\n",
      "Tiempo estimado: 2.4577248096466064 segundos\n",
      "Numero estimadores 91\n",
      "0.930383931096042\n",
      "Tiempo estimado: 2.787048101425171 segundos\n"
     ]
    }
   ],
   "source": [
    "bestNumberOfEstimatorsRFC = 0\n",
    "maxValueRFC = 0\n",
    "for i in range(1, 101, 10):\n",
    "    ini = time.time()\n",
    "    model = RandomForestClassifier(n_estimators=i)\n",
    "    scores = cross_val_score(model, X_Train, y_Train, cv=10, scoring='accuracy')\n",
    "    fin = time.time()\n",
    "    if scores.mean() > maxValueRFC:\n",
    "        maxValueRFC = scores.mean()\n",
    "        bestNumberOfEstimatorsRFC = i\n",
    "    print('Numero estimadores' , i)\n",
    "    print(scores.mean())\n",
    "    print('Tiempo estimado:', fin-ini, 'segundos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neuronas capa oculta:  10 alpha:  0.1 :   0.9541716362317108\n",
      "Tiempo estimado: 0.4636361598968506 segundos\n",
      "Neuronas capa oculta:  10 alpha:  0.01 :   0.9415095176896218\n",
      "Tiempo estimado: 0.26344728469848633 segundos\n",
      "Neuronas capa oculta:  10 alpha:  0.001 :   0.9313316057907335\n",
      "Tiempo estimado: 0.2175741195678711 segundos\n",
      "Neuronas capa oculta:  10 alpha:  1 :   0.9564423373295969\n",
      "Tiempo estimado: 1.1695330142974854 segundos\n",
      "Neuronas capa oculta:  10 alpha:  10 :   0.9530049958395649\n",
      "Tiempo estimado: 1.4305000305175781 segundos\n",
      "Neuronas capa oculta:  10 alpha:  100 :   0.9323558160180184\n",
      "Tiempo estimado: 1.3706681728363037 segundos\n",
      "Neuronas capa oculta:  20 alpha:  0.1 :   0.9656014921282867\n",
      "Tiempo estimado: 0.4382011890411377 segundos\n",
      "Neuronas capa oculta:  20 alpha:  0.01 :   0.9404417142412814\n",
      "Tiempo estimado: 0.23902249336242676 segundos\n",
      "Neuronas capa oculta:  20 alpha:  0.001 :   0.9404780623637181\n",
      "Tiempo estimado: 0.21792387962341309 segundos\n",
      "Neuronas capa oculta:  20 alpha:  1 :   0.9575931807913045\n",
      "Tiempo estimado: 1.3409931659698486 segundos\n",
      "Neuronas capa oculta:  20 alpha:  10 :   0.9495831367997283\n",
      "Tiempo estimado: 1.7598085403442383 segundos\n",
      "Neuronas capa oculta:  20 alpha:  100 :   0.9312586475737114\n",
      "Tiempo estimado: 1.6422860622406006 segundos\n",
      "Neuronas capa oculta:  30 alpha:  0.1 :   0.9644404340852061\n",
      "Tiempo estimado: 0.4898233413696289 segundos\n",
      "Neuronas capa oculta:  30 alpha:  0.01 :   0.9460573930342789\n",
      "Tiempo estimado: 0.2511589527130127 segundos\n",
      "Neuronas capa oculta:  30 alpha:  0.001 :   0.9371946336122965\n",
      "Tiempo estimado: 0.2465972900390625 segundos\n",
      "Neuronas capa oculta:  30 alpha:  1 :   0.958729544427668\n",
      "Tiempo estimado: 1.609074592590332 segundos\n",
      "Neuronas capa oculta:  30 alpha:  10 :   0.9484331142327995\n",
      "Tiempo estimado: 2.108501434326172 segundos\n",
      "Neuronas capa oculta:  30 alpha:  100 :   0.9335186067156929\n",
      "Tiempo estimado: 1.9259700775146484 segundos\n",
      "Neuronas capa oculta:  40 alpha:  0.1 :   0.9621781240490626\n",
      "Tiempo estimado: 0.5552201271057129 segundos\n",
      "Neuronas capa oculta:  40 alpha:  0.01 :   0.9439658162171731\n",
      "Tiempo estimado: 0.279341459274292 segundos\n",
      "Neuronas capa oculta:  40 alpha:  0.001 :   0.9462757125071161\n",
      "Tiempo estimado: 0.2783675193786621 segundos\n",
      "Neuronas capa oculta:  40 alpha:  1 :   0.9587696513795396\n",
      "Tiempo estimado: 1.5246517658233643 segundos\n",
      "Neuronas capa oculta:  40 alpha:  10 :   0.9495567097384174\n",
      "Tiempo estimado: 2.4504282474517822 segundos\n",
      "Neuronas capa oculta:  40 alpha:  100 :   0.9289330661783625\n",
      "Tiempo estimado: 2.1970267295837402 segundos\n",
      "Neuronas capa oculta:  50 alpha:  0.1 :   0.9586912609954268\n",
      "Tiempo estimado: 0.6380167007446289 segundos\n",
      "Neuronas capa oculta:  50 alpha:  0.01 :   0.9417584386375619\n",
      "Tiempo estimado: 0.3264589309692383 segundos\n",
      "Neuronas capa oculta:  50 alpha:  0.001 :   0.9429185849208273\n",
      "Tiempo estimado: 0.3196227550506592 segundos\n",
      "Neuronas capa oculta:  50 alpha:  1 :   0.9610688057135779\n",
      "Tiempo estimado: 1.8183321952819824 segundos\n",
      "Neuronas capa oculta:  50 alpha:  10 :   0.9495567097384174\n",
      "Tiempo estimado: 2.950072765350342 segundos\n",
      "Neuronas capa oculta:  50 alpha:  100 :   0.9312194523816549\n",
      "Tiempo estimado: 2.603604316711426 segundos\n",
      "Neuronas capa oculta:  60 alpha:  0.1 :   0.9622040339749199\n",
      "Tiempo estimado: 0.7297132015228271 segundos\n",
      "Neuronas capa oculta:  60 alpha:  0.01 :   0.9517879118849221\n",
      "Tiempo estimado: 0.3635132312774658 segundos\n",
      "Neuronas capa oculta:  60 alpha:  0.001 :   0.9484985235918779\n",
      "Tiempo estimado: 0.3462681770324707 segundos\n",
      "Neuronas capa oculta:  60 alpha:  1 :   0.9587696513795396\n",
      "Tiempo estimado: 1.6229217052459717 segundos\n",
      "Neuronas capa oculta:  60 alpha:  10 :   0.9506930733747812\n",
      "Tiempo estimado: 3.234276056289673 segundos\n",
      "Neuronas capa oculta:  60 alpha:  100 :   0.9300958568760368\n",
      "Tiempo estimado: 3.2149994373321533 segundos\n",
      "Neuronas capa oculta:  70 alpha:  0.1 :   0.9632256800647298\n",
      "Tiempo estimado: 0.9510006904602051 segundos\n",
      "Neuronas capa oculta:  70 alpha:  0.01 :   0.9463422465202985\n",
      "Tiempo estimado: 0.4125337600708008 segundos\n",
      "Neuronas capa oculta:  70 alpha:  0.001 :   0.9496372171624078\n",
      "Tiempo estimado: 0.3832428455352783 segundos\n",
      "Neuronas capa oculta:  70 alpha:  1 :   0.9587031173663574\n",
      "Tiempo estimado: 1.8016235828399658 segundos\n",
      "Neuronas capa oculta:  70 alpha:  10 :   0.9529922277088192\n",
      "Tiempo estimado: 3.7052228450775146 segundos\n",
      "Neuronas capa oculta:  70 alpha:  100 :   0.9300566616839804\n",
      "Tiempo estimado: 3.263338088989258 segundos\n",
      "Neuronas capa oculta:  80 alpha:  0.1 :   0.9564696062456823\n",
      "Tiempo estimado: 0.8001885414123535 segundos\n",
      "Neuronas capa oculta:  80 alpha:  0.01 :   0.9521010369118784\n",
      "Tiempo estimado: 0.40910792350769043 segundos\n",
      "Neuronas capa oculta:  80 alpha:  0.001 :   0.9518323749458009\n",
      "Tiempo estimado: 0.4154069423675537 segundos\n",
      "Neuronas capa oculta:  80 alpha:  1 :   0.9621522942673242\n",
      "Tiempo estimado: 1.7909722328186035 segundos\n",
      "Neuronas capa oculta:  80 alpha:  10 :   0.9506930733747812\n",
      "Tiempo estimado: 4.070882320404053 segundos\n",
      "Neuronas capa oculta:  80 alpha:  100 :   0.9312194523816549\n",
      "Tiempo estimado: 3.5666043758392334 segundos\n",
      "Neuronas capa oculta:  90 alpha:  0.1 :   0.959880478754412\n",
      "Tiempo estimado: 0.943079948425293 segundos\n",
      "Neuronas capa oculta:  90 alpha:  0.01 :   0.9565724571822273\n",
      "Tiempo estimado: 0.47813987731933594 segundos\n",
      "Neuronas capa oculta:  90 alpha:  0.001 :   0.9507843227546762\n",
      "Tiempo estimado: 0.4826991558074951 segundos\n",
      "Neuronas capa oculta:  90 alpha:  1 :   0.9599060150159033\n",
      "Tiempo estimado: 2.3360445499420166 segundos\n",
      "Neuronas capa oculta:  90 alpha:  10 :   0.9518294370111449\n",
      "Tiempo estimado: 4.706582307815552 segundos\n",
      "Neuronas capa oculta:  90 alpha:  100 :   0.9300566616839804\n",
      "Tiempo estimado: 4.149332046508789 segundos\n"
     ]
    }
   ],
   "source": [
    "bestNumNeuronas1 = 0\n",
    "bestAlpha = 0\n",
    "bestLearning_rate = 0\n",
    "maxValueMLP = 0\n",
    "for capa1 in range(10, 100, 10):\n",
    "    for j in [0.1, 0.01, 0.001, 1,10,100]:\n",
    "        ini = time.time()\n",
    "        model = MLPClassifier(hidden_layer_sizes=[capa1], activation='relu',solver='lbfgs',alpha=j,learning_rate='adaptive')\n",
    "        scores = cross_val_score(model, X_Train, y_Train, cv=10, scoring='accuracy')\n",
    "        fin = time.time()\n",
    "        if scores.mean() > maxValueMLP:\n",
    "            maxValueMLP = scores.mean()\n",
    "            bestNumNeuronas1 = capa1\n",
    "            bestAlpha = j\n",
    "            #bestLearning_rate = k\n",
    "        print('Neuronas capa oculta: ',capa1,'alpha: ',j,':  ',scores.mean())\n",
    "        print('Tiempo estimado:', fin-ini, 'segundos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8660030682080324\n"
     ]
    }
   ],
   "source": [
    "ini = time.time()\n",
    "model = GaussianNB([0.17,0.17,0.17,0.17,0.16,0.16])\n",
    "scores = cross_val_score(model,X_Train,y_Train,cv=10,scoring='accuracy')\n",
    "fin = time.time()\n",
    "print(scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C = 1 gamma = 0.1 : 0.6821686323845703\n",
      "Tiempo estimado: 1.4199812412261963 segundos\n",
      "C = 1 gamma = 0.01 : 0.9472552254702131\n",
      "Tiempo estimado: 0.5282444953918457 segundos\n",
      "C = 1 gamma = 0.001 : 0.9380910856020307\n",
      "Tiempo estimado: 0.6959824562072754 segundos\n",
      "C = 1 gamma = 1 : 0.18423258881449883\n",
      "Tiempo estimado: 1.4258594512939453 segundos\n",
      "C = 10 gamma = 0.1 : 0.7130961854414213\n",
      "Tiempo estimado: 1.424748420715332 segundos\n",
      "C = 10 gamma = 0.01 : 0.9586633039347326\n",
      "Tiempo estimado: 0.5217258930206299 segundos\n",
      "C = 10 gamma = 0.001 : 0.9506800014847101\n",
      "Tiempo estimado: 0.36510229110717773 segundos\n",
      "C = 10 gamma = 1 : 0.18423258881449883\n",
      "Tiempo estimado: 1.4579875469207764 segundos\n",
      "C = 100 gamma = 0.1 : 0.7130961854414213\n",
      "Tiempo estimado: 1.4278168678283691 segundos\n",
      "C = 100 gamma = 0.01 : 0.9586633039347326\n",
      "Tiempo estimado: 0.5039241313934326 segundos\n",
      "C = 100 gamma = 0.001 : 0.9633812942588598\n",
      "Tiempo estimado: 0.32999610900878906 segundos\n",
      "C = 100 gamma = 1 : 0.18423258881449883\n",
      "Tiempo estimado: 1.4770941734313965 segundos\n",
      "C = 1000 gamma = 0.1 : 0.7130961854414213\n",
      "Tiempo estimado: 1.4271166324615479 segundos\n",
      "C = 1000 gamma = 0.01 : 0.9586633039347326\n",
      "Tiempo estimado: 0.5051870346069336 segundos\n",
      "C = 1000 gamma = 0.001 : 0.9633812942588598\n",
      "Tiempo estimado: 0.32668042182922363 segundos\n",
      "C = 1000 gamma = 1 : 0.18423258881449883\n",
      "Tiempo estimado: 1.4982702732086182 segundos\n"
     ]
    }
   ],
   "source": [
    "mejorC = 0\n",
    "mejorGamma = 0\n",
    "maxPredictSVM = 0.0\n",
    "for i in [1, 10, 100, 1000]:\n",
    "    for j in [0.1, 0.01, 0.001, 1]:\n",
    "        ini = time.time()\n",
    "        model = SVC(kernel='rbf', C=i, gamma=j)\n",
    "        scores = cross_val_score(model, X_Train, y_Train, cv=10, scoring='accuracy')\n",
    "        fin = time.time()\n",
    "        if scores.mean() > maxPredictSVM:\n",
    "            maxPredictSVM = scores.mean()\n",
    "            mejorC = i\n",
    "            mejorGamma = j\n",
    "        print('C =', i, 'gamma =', j, ':', scores.mean())\n",
    "        print('Tiempo estimado:', fin-ini, 'segundos')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creacion y entrenamiento de algoritmos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelAda = AdaBoostClassifier(n_estimators=bestNumberOfEstimatorsAda)\n",
    "modelRandom = RandomForestClassifier(n_estimators=bestNumberOfEstimatorsRFC)\n",
    "modelSVC = SVC(kernel='rbf', C=mejorC, gamma=mejorGamma)\n",
    "modelGBC = GradientBoostingClassifier(random_state=1, n_estimators=bestNumberOfEstimatorsGBC)\n",
    "modelMLP =  MLPClassifier(hidden_layer_sizes=[bestNumNeuronas1], activation='relu',solver='lbfgs',alpha=bestAlpha,learning_rate='adaptative')\n",
    "modelNB = GaussianNB([0.17,0.17,0.17,0.17,0.16,0.16])\n",
    "modelAda.fit(X_Train,y_Train)\n",
    "modelRandom.fit(X_Train,y_Train)\n",
    "modelSVC.fit(X_Train,y_Train)\n",
    "modelGBC.fit(X_Train,y_Train)\n",
    "modelMLP.fit(X_Train,y_Train)\n",
    "modelNB.fit(X_Train,y_Train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pruebas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_Train, y_Train, random_state=0)\n",
    "class_names = ['Alegría', 'Tristeza', 'Miedo','Asco','Ira','Neutral']\n",
    "\n",
    "#y_pred = modelSVC.fit(X_train, y_train).predict(X_test)\n",
    "#y_pred = modelRandom.fit(X_train, y_train).predict(X_test)\n",
    "#y_pred = modelGBC.fit(X_train, y_train).predict(X_test)\n",
    "y_pred = modelNB.fit(X_train,y_train).predict(X_test)\n",
    "#\n",
    "\n",
    "precision, recall, f1Score, _ =precision_recall_fscore_support(y_test,y_pred)\n",
    "\n",
    "def plot_confusion_matrix(cm, classes, \n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()\n",
    "\n",
    "\n",
    "# Compute confusion matrix\n",
    "cnf_matrix = confusion_matrix(y_test, y_pred)\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=class_names,\n",
    "                      title='Confusion matrix, without normalization')\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=class_names, normalize=True,\n",
    "                      title='Normalized confusion matrix')\n",
    "\n",
    "plt.show()\n",
    "print('Precision: ',precision)\n",
    "print('Recall: ',recall)\n",
    "print('F1 Score: ',f1Score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por ultimo para realizar pruebas será necesario tratar los datos de prueba tal cuál se han tratado los datos de entrenamiento, por ello la función posee un parametro que indica que proceso se ha seguido y por lo tanto como a de tratar estos datos de prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformarDatos(datosPrueba, normalizacion, eliminacion):\n",
    "    datosPrueba = deleteNAWithMean(datosPrueba,mean)\n",
    "    if normalizacion == 1 or normalizacion == 4:\n",
    "        datosPrueba = scaler.transform(datosPrueba)\n",
    "    elif normalizacion == 2:\n",
    "        datosPrueba = normalizarPrueba(datosPrueba,mean,std)\n",
    "    else:\n",
    "        datosPrueba = normalize(datosPrueba, norm=norm,axis=0)\n",
    "    \n",
    "    if eliminacion == 2:\n",
    "        datosPrueba = datosPrueba.drop(['meanEnergy','minEnergy','rangeEnergy','stdEnergy','rangeAmplitude','minMFCC1','stdMFCC1','rangeMFCC2','rangeMFCC4','rangeMFCC12','maxPitch','stdPitch','quantileFormant1','quantileFormant2','rangeFormant2','quantileFormant3','quantileFormant4','rangeFormant4','rangeFormant5','quantileFormant5','stdAmplitude','articulationRate','meanIntensity'], axis=1)\n",
    "    elif eliminacion == 3:\n",
    "        datosPrueba = datosPrueba.drop(['minFormant1','maxFormant3','minHarmonicity','maxFormant5','rangeFormant2','rangeFormant1','maxMFCC6','maxFormant2','minFormant2','maxMFCC4','maxFormant4'], axis=1)\n",
    "    elif eliminacion ==4 :\n",
    "        datosPrueba = datosPrueba.drop(['minFormant1','maxFormant3','minHarmonicity','maxFormant5','rangeFormant1','maxMFCC6','maxFormant2','minFormant2','maxMFCC4','maxFormant4'], axis=1)\n",
    "    return datosPrueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def probar(filename,opcionNormalizacion,opcionEliminacion):\n",
    "    datosPrueba = pd.read_csv(filename, na_values='--undefined--')\n",
    "    nombres = datosPrueba.iloc[:,0]\n",
    "    datosPrueba = datosPrueba.iloc[:,1:]\n",
    "    transformarDatos(datosPrueba,opcionNormalizacion,opcionEliminacion)\n",
    "    datosPrueba = pd.DataFrame(datosPrueba)\n",
    "    resultados = pd.DataFrame(index = nombres)\n",
    "    resultados['AdaBoost'] = modelAda.predict(datosPrueba)\n",
    "    resultados['RandomForest'] = modelRandom.predict(datosPrueba)\n",
    "    resultados['SVM'] = modelSVC.predict(datosPrueba)\n",
    "    resultados['GradientBoosting'] = modelGBC.predict(datosPrueba)\n",
    "    resultados['NN'] = modelMLP.predict(datosPrueba)\n",
    "    resultados['NB'] = modelNB.predict(datosPrueba)\n",
    "    resultados.replace(0,\"Alegria\",inplace=True)\n",
    "    resultados.replace(1,\"Tristeza\",inplace=True)\n",
    "    resultados.replace(2,\"Miedo\",inplace=True)\n",
    "    resultados.replace(3,\"Asco\",inplace=True)\n",
    "    resultados.replace(4,\"Ira\",inplace=True)\n",
    "    resultados.replace(5,\"Neutral\",inplace=True)\n",
    "    print(resultados)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probar('sonidosPrueba.csv',opN,opE)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
