{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trabajo de fin de grado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se importan las librerias necesarias, en primer lugar algunas librerias para tratar con los tipos de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se importan tambien los algoritmos que se utilizaran más adelante para predecir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.model_selection  import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.naive_bayes import GaussianNB \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Librerias para el preprocesado y analisis de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Utilizados para normalizar\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "#Para la seleccion de variables\n",
    "import sklearn.feature_selection\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y otras de ayuda para plotear gráficas y mostrar métricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import itertools\n",
    "from sklearn.metrics import confusion_matrix, precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se comienza con la carga de los archivos .csv que han de encontrarse en el mismo directorio. Como se ve en el código cuando el valor no está definido aparece *--undefined--* y por ello esos valores son tratados como NA's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "audiosMezclados = pd.read_csv('sonidosMezclados.csv', na_values='--undefined--')\n",
    "audiosAleman = pd.read_csv('sonidosAleman.csv', na_values='--undefined--')\n",
    "audiosPropios = pd.read_csv('sonidosPropios.csv', na_values='--undefined--')\n",
    "audiosEspanol = pd.read_csv('sonidosEspanol.csv', na_values='--undefined--')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observando los cantidad de audios de las diferentes clases que se encuentran en cada uno de los dataframes, es fácil ver que en dataframe de audios en Español hay una presencia muy elevada de audios neutrales, por ello a continuación se eliminan varios de ellos de forma que finalmente todos estén presentes en la misma proporción"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neutrales = audiosEspanol[audiosEspanol['Result']==5]\n",
    "audiosEspanol = audiosEspanol.drop(np.arange((neutrales.index[0]+184),neutrales.index[-1]+1))\n",
    "audiosEspanol = audiosEspanol.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación se separan los dataframes en dos, uno con los datos de entrenamiento y otro con la etiqueta de cada ejemplo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_Mezclados = audiosMezclados.drop(['Result'], axis=1)\n",
    "y_Mezclados = audiosMezclados.Result\n",
    "\n",
    "X_Aleman = audiosAleman.drop(['Result'], axis=1)\n",
    "y_Aleman = audiosAleman.Result\n",
    "\n",
    "X_Propios = audiosPropios.drop(['Result'], axis=1)\n",
    "y_Propios = audiosPropios.Result\n",
    "\n",
    "X_Espanol = audiosEspanol.drop(['Result'], axis=1)\n",
    "y_Espanol = audiosEspanol.Result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se definen unas funciones utiles para normalizar los datos de entrenamiento y las pruebas que se realizan más adelante; junto con otras para eliminar valores faltantes con la media."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcularMediaYDesviacion(df):\n",
    "    mean = df.mean(numeric_only=1)\n",
    "    std = df.std(numeric_only=1)\n",
    "    return mean,std\n",
    "def normalizar(df, mean, std):\n",
    "    df = np.subtract(df,np.expand_dims(mean,axis=0))\n",
    "    df = np.divide(df,np.expand_dims(std,axis=0))\n",
    "    return df, mean, std\n",
    "def normalizarPrueba(df,mean,std):\n",
    "    df = np.subtract(df,np.expand_dims(mean,axis=0))\n",
    "    df = np.divide(df,np.expand_dims(std,axis=0))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deleteNAWithMean(df,mean):\n",
    "    for i in df.columns.values:\n",
    "        df[i].replace(np.nan, mean[i], inplace = True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dado que existen varios valores nulos, se reemplazan con la media de la columna a la que pertenecen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mediaMezclados, stdMezclados = calcularMediaYDesviacion(X_Mezclados)\n",
    "mediaAleman, stdAleman = calcularMediaYDesviacion(X_Aleman)\n",
    "mediaPropios, stdPropios = calcularMediaYDesviacion(X_Propios)\n",
    "mediaEspanol, stdEspanol = calcularMediaYDesviacion(X_Espanol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_Mezclados = deleteNAWithMean(X_Mezclados,mediaMezclados)\n",
    "X_Aleman = deleteNAWithMean(X_Aleman,mediaAleman)\n",
    "X_Propios = deleteNAWithMean(X_Propios,mediaPropios)\n",
    "X_Espanol = deleteNAWithMean(X_Espanol,mediaEspanol)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para realizar un analisis sobre la correlación que existe entre las variables, se muestran algunos mapas de calor en los que existe al menos un par de variables con una correlación absoluta mayor a 0.8, para después valorar la eliminación de alguna de esas variables con la finalidad de reducir los datos de entrenamiento necesarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "correlation = X_Mezclados.corr()\n",
    "limites = [0,5,10,15,21,23,28,33,38,43,48,53,58,63,68,73,78,83,88,94,100,106,112,118]\n",
    "for i in range(0,len(limites)-1):\n",
    "    for j in range(i,len(limites)-1):\n",
    "        correlation = correlation.iloc[limites[i]:limites[i+1],limites[j]:limites[j+1]]\n",
    "        if(np.any(correlation<-0.8) or np.any(correlation>0.8)):\n",
    "            plt.figure()\n",
    "            sns.heatmap(correlation, xticklabels = correlation.columns.values,\n",
    "                        yticklabels=correlation.index.values, vmin=-1,vmax=1,annot=True)\n",
    "            plt.title('Feature correlations')\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(correlation.columns.values[0] + correlation.index.values[0])\n",
    "        correlation = X_Mezclados.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalizar datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se definen varios métodos para normalizar con el objetivo de poder comparar los resultados que producen cada una de ellas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalización utilizando el método MinMax, que convierte los valores a un intervalo de 0-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "scalerMezclados = scaler.fit(X_Mezclados)\n",
    "scalerAleman = scaler.fit(X_Aleman)\n",
    "scalerPropios = scaler.fit(X_Propios)\n",
    "scalerEspanol = scaler.fit(X_Espanol)\n",
    "minMaxScaled_Mezclados = scalerMezclados.transform(X_Mezclados)\n",
    "minMaxScaled_Aleman = scalerAleman.transform(X_Aleman)\n",
    "minMaxScaled_Propios = scalerPropios.transform(X_Propios)\n",
    "minMaxScaled_Espanol = scalerEspanol.transform(X_Espanol)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalización ZScore, para transformar los datos a una normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalizar_Mezclados = normalizar(X_Mezclados,meanMezclados,stdMezclados)\n",
    "normalizar_Aleman = normalizar(X_Aleman,meanAleman,stdAleman)\n",
    "normalizar_Propios = normalizar(X_Propios,meanPropios,stdPropios)\n",
    "normalizar_Espanol = normalizar(X_Espanol,meanEspanol,stdEspanol)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalización utilizando la norma l1, l2 o max, según se especifique en los argumentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_Mezclados, normMezclados = normalize(X_Mezclados, norm = 'l2',axis=0,return_norm=True)\n",
    "normalized_Aleman, normAleman = normalize(X_Aleman, norm='l2',axis=0,return_norm=True)\n",
    "normalized_Propios, normPropios = normalize(X_Propios, norm='l2',axis=0,return_norm=True)\n",
    "normalized_Espanol, normEspanol = normalize(X_Espanol, norm='l2',axis=0,return_norm=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalización realizada con el rango interquartílico, de forma que sea más robusta frente a valores atípicos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "robustScaler = RobustScaler()\n",
    "robustScalerMezclados = robustScaler.fit(X_Mezclados)\n",
    "robustScalerAleman = robustScaler.fit(X_Aleman)\n",
    "robustScalerPropios = robustScaler.fit(X_Propios)\n",
    "robustScalerEspanol = robustScaler.fit(X_Espanol)\n",
    "robustScaled_Mezclados = robustScalerMezclados.transform(X_Mezclados)\n",
    "robustScaled_Aleman = robustScalerAleman.transform(X_Aleman)\n",
    "robustScaled_Propios = robustScalerPropios.transform(X_Propios)\n",
    "robustScaled_Espanol = robustScalerEspanol.transform(X_Espanol)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seleccionar datos para entrenar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para facilitar la selección de los datos se ponen a disposicion asignaciones de todas las normalizaciones de los datos, de forma que bastará con descomentar las asignaciones que se quieran realizar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#opN=1\n",
    "#X_MezcladosNor, scalerM = pd.DataFrame(minMaxScaled_Mezclados, columns = X_Mezclados.columns.values), scalerMezclados\n",
    "#X_AlemanNor, scalerA = pd.DataFrame(minMaxScaled_Aleman, columns = X_Aleman.columns.values), scalerAleman\n",
    "#X_PropiosNor, scalerP = pd.DataFrame(minMaxScaled_Propios, columns = X_Propios.columns.values), scalerPropios\n",
    "#X_EspanolNor, scalerE = pd.DataFrame(minMaxScaled_Espanol, columns = X_Espanol.columns.values), scalerEspanol\n",
    "\n",
    "opN=2\n",
    "X_MezcladosNor= pd.DataFrame(normalizar_Mezclados)\n",
    "X_AlemanNor= pd.DataFrame(normalizar_Aleman)\n",
    "X_PropiosNor= pd.DataFrame(normalizar_Propios)\n",
    "X_EspanolNor= pd.DataFrame(normalizar_Espanol)\n",
    "\n",
    "#opN=3\n",
    "#X_MezcladosNor = pd.DataFrame(normalized_Mezclados, columns = X_Mezclados.columns.values)\n",
    "#X_AlemanNor = pd.DataFrame(normalized_Aleman, columns = X_Aleman.columns.values)\n",
    "#X_PropiosNor = pd.DataFrame(normalized_Propios, columns = X_Propios.columns.values)\n",
    "#X_EspanolNor = pd.DataFrame(normalized_Espanol, columns = X_Espanol.columns.values)\n",
    "\n",
    "#opN=4\n",
    "#X_MezcladosNor, scalerM = pd.DataFrame(robustScaled_Mezclados, columns = X_Mezclados.columns.values), robustScalerMezclados\n",
    "#X_AlemanNor, scalerA = pd.DataFrame(robustScaled_Aleman, columns = X_Aleman.columns.values), robustScalerAleman\n",
    "#X_PropiosNor, scalerP = pd.DataFrame(robustScaled_Propios, columns = X_Propios.columns.values), robustScalerPropios\n",
    "#X_EspanolNor, scalerE = pd.DataFrame(robustScaled_Espanol, columns = X_Espanol.columns.values), robustScalerEspanol"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seleccion de variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para seleccionar variables a eliminar, a parte de utilizar el analisis que se hizo anteriormente en base a las correlaciones; se lanza un algoritmo de decisión para que establezca un orden en la importancia de las variables. Así las variables que el arbol considere menos importantes se podrán eliminar, al igual que las que considere más importantes se procurará mantenerlas "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model = ExtraTreesClassifier(n_estimators=90)\n",
    "model = model.fit(X_MezcladosNor,y_Mezclados)\n",
    "featureImportance = model.feature_importances_\n",
    "print(featureImportance)\n",
    "featureImportance = np.argsort(featureImportance)\n",
    "print(featureImportance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tras entrenar con el arbol, se implementa un metodo que devuelve las variables más o menos importantes, en función del signo del parámetro de entrada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def moreImportantFeatures(how_many, classifier, inputData, outputData):\n",
    "    model = classifier.fit(inputData,outputData)\n",
    "    featureImportance = model.feature_importances_\n",
    "    orderImportance = np.argsort(featureImportance)\n",
    "    if(how_many >=0):\n",
    "        for i in range(1,how_many):\n",
    "            print(inputData.columns.values[orderImportance[-i]])\n",
    "    else:\n",
    "        for i in range(1,-how_many):\n",
    "            print(inputData.columns.values[orderImportance[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moreImportantFeatures(-20, ExtraTreesClassifier(n_estimators=90),X_AlemanNor,y_Aleman)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación se crean varios conjuntos eliminando algunas variables. En uno de ellos eliminamos variables que tenian más de un 0.8 de correlación absoluta; otro eliminando aquellas variables que el arbol de decisión determina que son menos influyentes y un ultimo eliminando variables que cumplan alguna de las condiciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_MezcladosNor80 X_MezcladosNor.drop(['meanEnergy','minEnergy','rangeEnergy','stdEnergy','rangeAmplitude','minMFCC1','stdMFCC1','rangeMFCC2','rangeMFCC4','rangeMFCC12','maxPitch','stdPitch','quantileFormant1','quantileFormant2','rangeFormant2','quantileFormant3','quantileFormant4','rangeFormant4','rangeFormant5','quantileFormant5','stdAmplitude','articulationRate','meanIntensity'], axis=1)\n",
    "X_AlemanNor80 = X_AlemanNor.drop(['meanEnergy','minEnergy','rangeEnergy','stdEnergy','rangeAmplitude','minMFCC1','stdMFCC1','rangeMFCC2','rangeMFCC4','rangeMFCC12','maxPitch','stdPitch','quantileFormant1','quantileFormant2','rangeFormant2','quantileFormant3','quantileFormant4','rangeFormant4','rangeFormant5','quantileFormant5','stdAmplitude','articulationRate','meanIntensity'], axis=1)\n",
    "X_EspanolNor80 = X_EspanolNor.drop(['meanEnergy','minEnergy','rangeEnergy','stdEnergy','rangeAmplitude','minMFCC1','stdMFCC1','rangeMFCC2','rangeMFCC4','rangeMFCC12','maxPitch','stdPitch','quantileFormant1','quantileFormant2','rangeFormant2','quantileFormant3','quantileFormant4','rangeFormant4','rangeFormant5','quantileFormant5','stdAmplitude','articulationRate','meanIntensity'], axis=1)\n",
    "X_PropiosNor80 = X_PropiosNor.drop(['meanEnergy','minEnergy','rangeEnergy','stdEnergy','rangeAmplitude','minMFCC1','stdMFCC1','rangeMFCC2','rangeMFCC4','rangeMFCC12','maxPitch','stdPitch','quantileFormant1','quantileFormant2','rangeFormant2','quantileFormant3','quantileFormant4','rangeFormant4','rangeFormant5','quantileFormant5','stdAmplitude','articulationRate','meanIntensity'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_MezcladosNorLessImportant = X_MezcladosNor.drop(['minFormant1','maxFormant3','minHarmonicity','maxFormant5','rangeFormant2','rangeFormant1','maxMFCC6','maxFormant2','minFormant2','maxMFCC4','maxFormant4'], axis=1)\n",
    "X_AlemanNorLessImportant = X_AlemanNor.drop(['minFormant1','maxFormant3','minHarmonicity','maxFormant5','rangeFormant2','rangeFormant1','maxMFCC6','maxFormant2','minFormant2','maxMFCC4','maxFormant4'], axis=1)\n",
    "X_EspanolNorLessImportant = X_EspanolNor.drop(['minFormant1','maxFormant3','minHarmonicity','maxFormant5','rangeFormant2','rangeFormant1','maxMFCC6','maxFormant2','minFormant2','maxMFCC4','maxFormant4'], axis=1)\n",
    "X_PropiosNorLessImportant = X_PropiosNor.drop(['minFormant1','maxFormant3','minHarmonicity','maxFormant5','rangeFormant2','rangeFormant1','maxMFCC6','maxFormant2','minFormant2','maxMFCC4','maxFormant4'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_MezcladosNor80LessImportant = X_MezcladosNor80.drop(['minFormant1','maxFormant3','minHarmonicity','maxFormant5','rangeFormant1','maxMFCC6','maxFormant2','minFormant2','maxMFCC4','maxFormant4'], axis=1)\n",
    "X_AlemanNor80LessImportant = X_AlemanNor80.drop(['minFormant1','maxFormant3','minHarmonicity','maxFormant5','rangeFormant1','maxMFCC6','maxFormant2','minFormant2','maxMFCC4','maxFormant4'], axis=1)\n",
    "X_EspanolNor80LessImportant = X_EspanolNor80.drop(['minFormant1','maxFormant3','minHarmonicity','maxFormant5','rangeFormant1','maxMFCC6','maxFormant2','minFormant2','maxMFCC4','maxFormant4'], axis=1)\n",
    "X_PropiosNor80LessImportant = X_PropiosNor80.drop(['minFormant1','maxFormant3','minHarmonicity','maxFormant5','rangeFormant1','maxMFCC6','maxFormant2','minFormant2','maxMFCC4','maxFormant4'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def histogramas(data):\n",
    "    for nameOfColumn in data.columns.values:\n",
    "        data.hist(nameOfColumn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "histogramas(X_Mezclados)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nuevamente tras finalizar esta fase se dispone una serie de opciones para determinar con que conjunto se procedera a la eliminación de outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opE =1\n",
    "X_MezcladosReduced = X_MezcladosNor\n",
    "X_AlemanReduced = X_AlemanNor\n",
    "X_PropiosReduced = X_PropiosNor\n",
    "X_EspanolReduced = X_EspanolNor\n",
    "\n",
    "#opE=2\n",
    "#X_MezcladosReduced = X_MezcladosNor80\n",
    "#X_AlemanReduced = X_AlemanNor80\n",
    "#X_PropiosReduced = X_PropiosNor80\n",
    "#X_EspanolReduced = X_EspanolNor80\n",
    "\n",
    "#opE=3\n",
    "#X_MezcladosReduced = X_MezcladosNorLessImportant\n",
    "#X_AlemanReduced = X_AlemanNorLessImportant\n",
    "#X_PropiosReduced = X_PropiosNorLessImportant\n",
    "#X_EspanolReduced = X_EspanolNorLessImportant\n",
    "\n",
    "#opE=4\n",
    "#X_MezcladosReduced = X_MezcladosNor80LessImportant\n",
    "#X_AlemanReduced = X_AlemanNor80LessImportant\n",
    "#X_PropiosReduced = X_PropiosNor80LessImportant\n",
    "#X_EspanolReduced = X_EspanolNor80LessImportant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supresion de outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para eliminar los outliers se define una función que calcula el primer y tercer cuartil y con ello el rango intercuartílico. En este caso, dado que el porcentaje de outliers inicial era demasiado elevado, el valor que multiplica al rango intercualtílico ha sido aumentado para ser más flexibles a la hora de determinar si un valor es atípico."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deleteOutliers(data,outdata):\n",
    "    lowerOutliers = np.array([False])\n",
    "    upperOutliers = np.array([False])\n",
    "    for i in data.columns.values:\n",
    "        quartiles = data[i].quantile([0.25,0.75])\n",
    "        IQR = quartiles[0.75]-quartiles[0.25]\n",
    "        lowerLimit = quartiles[0.25] - 2.5*IQR\n",
    "        upperLimit = quartiles[0.75] + 2.5*IQR\n",
    "        lowerOutliersAux = data[i].values < lowerLimit\n",
    "        upperOutliersAux = data[i].values > upperLimit\n",
    "        lowerOutliers = np.logical_or(lowerOutliers, lowerOutliersAux)\n",
    "        upperOutliers = np.logical_or(upperOutliers, upperOutliersAux)\n",
    "    outliers = np.logical_or(lowerOutliers, upperOutliers)\n",
    "    data = data.drop(np.where(outliers)[0])\n",
    "    outdata = outdata.drop(np.where(outliers)[0])\n",
    "    data = data.reset_index(drop=True)\n",
    "    outdata = outdata.reset_index(drop=True)\n",
    "    return data, outdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_MezcladosNorWithoutOutliers,y_MezcladosWithoutOutliers=deleteOutliers(X_MezcladosReduced,y_Mezclados)\n",
    "X_AlemanNorWithoutOutliers, y_AlemanWithoutOutliers=deleteOutliers(X_AlemanReduced, y_Aleman)\n",
    "X_PropiosNorWithoutOutliers, y_PropiosWithoutOutliers=deleteOutliers(X_PropiosReduced, y_Propios)\n",
    "X_EspanolNorWithoutOutliers, y_EspanolWithoutOutliers=deleteOutliers(X_EspanolReduced, y_Espanol)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente se decide con que conjunto de datos se procede a entrenar y validar los algoritmos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_Train, y_Train, mean, std = X_MezcladosReduced,y_Mezclados, meanMezclados, stdMezclados\n",
    "#X_Train, y_Train, mean, std = X_AlemanReduced,y_Aleman, meanAleman, stdAleman\n",
    "#X_Train, y_Train, mean, std = X_PropiosReduced,y_Propios, meanPropios, stdPropios\n",
    "#X_Train, y_Train, mean, std = X_EspanolReduced,y_Espanol, meanEspanol, stdEspanol\n",
    "#X_Train, y_Train, mean, std = X_MezcladosNorWithoutOutliers,y_MezcladosWithoutOutliers, meanMezclados, stdMezclados\n",
    "#X_Train, y_Train, mean, std = X_AlemanNorWithoutOutliers,y_AlemanWithoutOutliers, meanAleman, stdAleman\n",
    "#X_Train, y_Train, mean, std = X_PropiosNorWithoutOutliers,y_PropiosWithoutOutliers, meanPropios, stdPropios\n",
    "#X_Train, y_Train, mean, std = X_EspanolNorWithoutOutliers,y_EspanolWithoutOutliers, meanEspanol, stdEspanol\n",
    "\n",
    "#En caso de utilizar un metodo de normalización que se realice mediante un scaler\n",
    "#scaler = scalerM\n",
    "#scaler = scalerA\n",
    "#scaler = scalerP\n",
    "#scaler = scalerE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pruebas para determinar valores adecuados para cada algoritmo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bestNumberOfEstimatorsAda = 0\n",
    "maxValueABC = 0.0\n",
    "for i in range(1, 101, 10):\n",
    "    ini = time.time()\n",
    "    model = AdaBoostClassifier(n_estimators=i)\n",
    "    scores = cross_val_score(model, X_Train, y_Train, cv=10, scoring='accuracy')\n",
    "    fin = time.time()\n",
    "    if scores.mean() > maxValueABC:\n",
    "        maxValueABC = scores.mean()\n",
    "        bestNumberOfEstimatorsAda = i\n",
    "    print('Numero estimadores', i)\n",
    "    print(scores.mean())\n",
    "    print('Tiempo estimado:', fin-ini, 'segundos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bestNumberOfEstimatorsGBC = 0\n",
    "maxValueGBC = 0.0\n",
    "for i in range(1, 101, 10):\n",
    "    ini = time.time()\n",
    "    model = GradientBoostingClassifier(random_state=1, n_estimators=i)\n",
    "    scores = cross_val_score(model, X_Train, y_Train, cv=10, scoring='accuracy')\n",
    "    fin = time.time()\n",
    "    if scores.mean() > maxValueGBC:\n",
    "        maxValueGBC = scores.mean()\n",
    "        bestNumberOfEstimatorsGBC = i\n",
    "    print('Numero estimadores' , i)\n",
    "    print(scores.mean())\n",
    "    print('Tiempo estimado:', fin-ini, 'segundos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bestNumberOfEstimatorsRFC = 0\n",
    "maxValueRFC = 0\n",
    "for i in range(1, 101, 10):\n",
    "    ini = time.time()\n",
    "    model = RandomForestClassifier(n_estimators=i)\n",
    "    scores = cross_val_score(model, X_Train, y_Train, cv=10, scoring='accuracy')\n",
    "    fin = time.time()\n",
    "    if scores.mean() > maxValueRFC:\n",
    "        maxValueRFC = scores.mean()\n",
    "        bestNumberOfEstimatorsRFC = i\n",
    "    print('Numero estimadores' , i)\n",
    "    print(scores.mean())\n",
    "    print('Tiempo estimado:', fin-ini, 'segundos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bestNumNeuronas1 = 0\n",
    "bestNumNeuronas2 = 0\n",
    "bestNumNeuronas3 = 0\n",
    "bestAlpha = 0\n",
    "bestLearning_rate = 0\n",
    "maxValueMLP = 0\n",
    "for capa1 in range(10, 100, 10):\n",
    "    #for capa2 in range(10, 100, 10):\n",
    "     #   for capa3 in range(10, 100, 10):\n",
    "            for j in [0.1, 0.01, 0.001, 1,10,100]:\n",
    "                ini = time.time()\n",
    "                model = MLPClassifier(hidden_layer_sizes=[capa1], activation='relu',solver='lbfgs',alpha=j,learning_rate='adaptative')\n",
    "                scores = cross_val_score(model, X_Train, y_Train, cv=10, scoring='accuracy')\n",
    "                fin = time.time()\n",
    "                if scores.mean() > maxValueMLP:\n",
    "                    maxValueMLP = scores.mean()\n",
    "                    bestNumNeuronas1 = capa1\n",
    "                    #bestNumNeuronas2 = capa2\n",
    "                    #bestNumNeuronas3 = capa3\n",
    "                    bestAlpha = j\n",
    "                    #bestLearning_rate = k\n",
    "                print('Neuronas capa oculta: ',capa1,'alpha: ',j,':  ',scores.mean())\n",
    "                print('Tiempo estimado:', fin-ini, 'segundos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ini = time.time()\n",
    "model = GaussianNB([0.17,0.17,0.17,0.17,0.16,0.16])\n",
    "scores = cross_val_score(model,X_Train,y_Train,cv=10,scoring='accuracy')\n",
    "fin = time.time()\n",
    "print(scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mejorC = 0\n",
    "mejorGamma = 0\n",
    "maxPredictSVM = 0.0\n",
    "for i in [1, 10, 100, 1000]:\n",
    "    for j in [0.1, 0.01, 0.001, 1]:\n",
    "        ini = time.time()\n",
    "        model = SVC(kernel='rbf', C=i, gamma=j)\n",
    "        scores = cross_val_score(model, X_Train, y_Train, cv=10, scoring='accuracy')\n",
    "        fin = time.time()\n",
    "        if scores.mean() > maxPredictSVM:\n",
    "            maxPredictSVM = scores.mean()\n",
    "            mejorC = i\n",
    "            mejorGamma = j\n",
    "        print('C =', i, 'gamma =', j, ':', scores.mean())\n",
    "        print('Tiempo estimado:', fin-ini, 'segundos')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creacion y entrenamiento de algoritmos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelAda = AdaBoostClassifier(n_estimators=bestNumberOfEstimatorsAda)\n",
    "modelRandom = RandomForestClassifier(n_estimators=bestNumberOfEstimatorsRFC)\n",
    "modelSVC = SVC(kernel='rbf', C=mejorC, gamma=mejorGamma)\n",
    "modelGBC = GradientBoostingClassifier(random_state=1, n_estimators=bestNumberOfEstimatorsGBC)\n",
    "modelMLP =  MLPClassifier(hidden_layer_sizes=[bestNumNeuronas1], activation='relu',solver='lbfgs',alpha=bestAlpha,learning_rate='adaptative')\n",
    "modelNB = GaussianNB([0.17,0.17,0.17,0.17,0.16,0.16])\n",
    "modelAda.fit(X_Train,y_Train)\n",
    "modelRandom.fit(X_Train,y_Train)\n",
    "modelSVC.fit(X_Train,y_Train)\n",
    "modelGBC.fit(X_Train,y_Train)\n",
    "modelMLP.fit(X_Train,y_Train)\n",
    "modelNB.fit(X_Train,y_Train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pruebas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_Train, y_Train, random_state=0)\n",
    "class_names = ['Alegría', 'Tristeza', 'Miedo','Asco','Ira','Neutral']\n",
    "\n",
    "#y_pred = modelSVC.fit(X_train, y_train).predict(X_test)\n",
    "#y_pred = modelRandom.fit(X_train, y_train).predict(X_test)\n",
    "#y_pred = modelGBC.fit(X_train, y_train).predict(X_test)\n",
    "y_pred = modelNB.fit(X_train,y_train).predict(X_test)\n",
    "#\n",
    "\n",
    "precision, recall, f1Score, _ =precision_recall_fscore_support(y_test,y_pred)\n",
    "\n",
    "def plot_confusion_matrix(cm, classes, \n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()\n",
    "\n",
    "\n",
    "# Compute confusion matrix\n",
    "cnf_matrix = confusion_matrix(y_test, y_pred)\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=class_names,\n",
    "                      title='Confusion matrix, without normalization')\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=class_names, normalize=True,\n",
    "                      title='Normalized confusion matrix')\n",
    "\n",
    "plt.show()\n",
    "print('Precision: ',precision)\n",
    "print('Recall: ',recall)\n",
    "print('F1 Score: ',f1Score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por ultimo para realizar pruebas será necesario tratar los datos de prueba tal cuál se han tratado los datos de entrenamiento, por ello la función posee un parametro que indica que proceso se ha seguido y por lo tanto como a de tratar estos datos de prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformarDatos(datosPrueba, normalizacion, eliminacion):\n",
    "    if normalizacion == 1 or normalizacion == 4:\n",
    "        datosPrueba = scaler.transform(datosPrueba)\n",
    "    else if normalizacion == 2:\n",
    "        datosPrueba = normalizarPrueba(datosPrueba,mean,std)\n",
    "    else datosPrueba = normalize(datosPrueba, norm=norm,axis=0)\n",
    "    \n",
    "    if eliminacion == 2:\n",
    "        datosPrueba = datosPrueba.drop(['meanEnergy','minEnergy','rangeEnergy','stdEnergy','rangeAmplitude','minMFCC1','stdMFCC1','rangeMFCC2','rangeMFCC4','rangeMFCC12','maxPitch','stdPitch','quantileFormant1','quantileFormant2','rangeFormant2','quantileFormant3','quantileFormant4','rangeFormant4','rangeFormant5','quantileFormant5','stdAmplitude','articulationRate','meanIntensity'], axis=1)\n",
    "    else if eliminacion == 3:\n",
    "        datosPrueba = datosPrueba.drop(['minFormant1','maxFormant3','minHarmonicity','maxFormant5','rangeFormant2','rangeFormant1','maxMFCC6','maxFormant2','minFormant2','maxMFCC4','maxFormant4'], axis=1)\n",
    "    else if eliminacion ==4 :\n",
    "        datosPrueba = datosPrueba.drop(['minFormant1','maxFormant3','minHarmonicity','maxFormant5','rangeFormant1','maxMFCC6','maxFormant2','minFormant2','maxMFCC4','maxFormant4'], axis=1)\n",
    "    return datosPrueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def probar(filename,opcionNormalizacion,opcionEliminacion):\n",
    "    datosPrueba = pd.read_csv(filename, na_values='--undefined--')\n",
    "    nombres = datosPrueba.iloc[:,0]\n",
    "    datosPrueba = datosPrueba.iloc[:,1:]\n",
    "    datosPrueba = deleteNAWithMean2(datosPrueba,meanEspanol)\n",
    "    #datosPrueba = scalerMezclados.transform(datosPrueba)\n",
    "    datosPrueba = normalizarPrueba(datosPrueba,meanEspanol,stdEspanol)\n",
    "    #datosPrueba = datosPrueba.drop(['meanEnergy','minEnergy','rangeEnergy','stdEnergy','rangeAmplitude','minMFCC1','stdMFCC1','rangeMFCC2','rangeMFCC4','rangeMFCC12','maxPitch','stdPitch','quantileFormant1','quantileFormant2','rangeFormant2','quantileFormant3','quantileFormant4','rangeFormant4','rangeFormant5','quantileFormant5','stdAmplitude','articulationRate','meanIntensity'], axis=1)\n",
    "    \n",
    "    \n",
    "    datosPrueba = pd.DataFrame(datosPrueba)\n",
    "    resultados = pd.DataFrame(index = nombres)\n",
    "    resultados['AdaBoost'] = modelAda.predict(datosPrueba)\n",
    "    resultados['RandomForest'] = modelRandom.predict(datosPrueba)\n",
    "    resultados['SVM'] = modelSVC.predict(datosPrueba)\n",
    "    resultados['GradientBoosting'] = modelGBC.predict(datosPrueba)\n",
    "    resultados['NN'] = modelMLP.predict(datosPrueba)\n",
    "    resultados['NB'] = modelNB.predict(datosPrueba)\n",
    "    resultados.replace(0,\"Alegria\",inplace=True)\n",
    "    resultados.replace(1,\"Tristeza\",inplace=True)\n",
    "    resultados.replace(2,\"Miedo\",inplace=True)\n",
    "    resultados.replace(3,\"Asco\",inplace=True)\n",
    "    resultados.replace(4,\"Ira\",inplace=True)\n",
    "    resultados.replace(5,\"Neutral\",inplace=True)\n",
    "    print(resultados)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probar('sonidosPrueba.csv',opN,opE)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
